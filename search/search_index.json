{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to UPPMAX for Deep Learning! Let's get started! \ud83d\ude80"},{"location":"DL_exercises/","title":"DL Exercises","text":"<p>Info</p> <p>We put some exercises here for you, if you want to get some more hands-on.</p>"},{"location":"DL_exercises/#prepare-your-project-folder","title":"Prepare your project folder","text":"Make arrangements for the new project <ul> <li>Find your way into your project uppmax2025-3-5 by logging in to Rackham by ThinLinc/ssh/VSCode.</li> <li> <p>Go to private folder and make an empty folder with your name under <code>private</code> or <code>nobackup/private</code></p> Answer <p><code>ssh jayan@rackham.uppmax.uu.se</code> <code>ssh -X jayan@rackham.uppmax.uu.se</code> <code>cd /proj/uppmax2025-3-5</code> <code>mkdir</code> </p> </li> <li> <p>Copy contents from <code>uppmax_workshop</code> to your private folder. </p> </li> </ul>"},{"location":"DL_exercises/#create-environment","title":"Create environment","text":"Use venv or conda to prepare your environment <ul> <li> <p>Module load python and use venv to create a environment with pytorch, transformers and jupyter.</p> Answer <pre><code>module load python/3.12.7\nwhich python\npython -V\npython -m venv torch_env\n</code></pre> <p>If using conda, follow instructions from \"Use isolated environments\" section.</p> </li> <li> <p>Activate your env and install desired packages.</p> </li> </ul>"},{"location":"DL_exercises/#using-the-compute-nodes","title":"Using the compute nodes","text":"Submit a Slurm job <ul> <li>Submit <code>sentiment_analysis.py</code> with appropriate flags for its slurm job.</li> </ul> Answer <ul> <li>edit a file using you prefered editor, named <code>sentiment_analysis_batch.sh</code>, for example, with the content</li> </ul> <pre><code>    #!/bin/bash -l\n\n    #SBATCH -A uppmax2025-3-5\n    #SBATCH -p node\n    #SBATCH -N 1\n    #SBATCH -t 01:00:00\n    #SBATCH -J sentiment_analysis\n    #SBATCH -M snowy\n    #SBATCH --gres=gpu:1\n\n    source .....\n\n    python -c \"import torch; print(torch.__version__); print(torch.version.cuda); print(torch.cuda.get_device_properties(0)); print(torch.randn(1).cuda())\"\n\n    echo \"running sentiment_analysis.py\"\n\n    python .....sentiment_analysis.py\n</code></pre> <ul> <li> <p>make the job script executable, if not already. <pre><code>$ chmod a+x sentiment_analysis_batch.sh\n</code></pre></p> </li> <li> <p>submit the job <pre><code>$ sbatch sentiment_analysis_batch.sh\n</code></pre></p> </li> </ul> <ul> <li> <p>Similarly run <code>sentiment_analysis.ipynb</code> jupyter notebook. Add matplotblib and seaborn to your environment. Start a jupyter server on snowy compute node and then tunnel to the host from your local.</p> Answer <ul> <li>Install matplotlib and seaborn and start an interactive snowy session:  </li> </ul> <p><pre><code>    source torch_env/bin/acivate\n    pip install matplotlib seaborn\n    interactive -A uppmax2025-3-5 -M snowy -p node -N 1 -t 1:01:00 --gres=gpu:1\n    source torch_env/bin/acivate\n    jupyter notebook --ip 0.0.0.0 --no-browser\n</code></pre> - Then tunnel: <code>ssh -L 8888:s123:8888 username@rackham.uppmax.uu.se</code></p> <ul> <li>Copy the localhost url and paste it in your browser</li> </ul> </li> </ul>"},{"location":"DL_exercises/#cache-management","title":"Cache management","text":"<ul> <li>By default HF will install the models and temp files to yur <code>$HOME</code> folde which is rather limited to 32 GB and 300k files. </li> <li>To avoid that, you can set <code>HF_HOME</code> and <code>HF_HUB_CACHE</code> variables to your project folder. Follow instructions on HuggingFace's documentation.</li> </ul>"},{"location":"VSCode/","title":"Setting up VSCode for Rackham and Snowy","text":"<p>Info</p> <ul> <li>You can run VSCode on your local and still be able to work with modules loaded or environment created on Rackham. </li> <li>Similarly it is possible to take advantage of Snowy GPUs meanwhile developing on your local computer. </li> </ul> <p>Latest VSCode does not work</p> <p>VSCode versions 1.99 and newer no longer support the operating system used on Rackham. Downgrade VSCode to version 1.98 or earlier to use with Rackham.</p> <p>This will not be needed with the upcoming Rackham replacement cluster Pelle.</p>"},{"location":"VSCode/#1-connect-your-local-vscode-to-vscode-server-running-on-rackham","title":"1. Connect your local VSCode to VSCode server running on Rackham","text":"<p>A step by step approach is available on UPPMAX VSCode documentation. </p> <p>When you first establish the ssh connection to Rackham, your VSCode server directory <code>.vscode-server</code> will be created in your home folder <code>/home/[username]</code>. </p>"},{"location":"VSCode/#2-install-and-manage-extensions-on-remote-vscode-server","title":"2. Install and manage Extensions on remote VSCode server","text":"<p>By default all the VSCode extensions will get installed on your home folder <code>/home/[username]</code>. Due to less storage quota on home folder <code>32 GB, 300k files</code>, can quickly fill up with extensions and other file operations. The default installtion path for VSCode extensions can however be changed to your project folder which have way more storage space and file count capacity, <code>1TB, 1M files</code>.</p>"},{"location":"VSCode/#21-manage-extensions","title":"2.1. Manage Extensions","text":"<p>Go to Command Palette <code>Ctrl+Shift+P</code> or <code>F1</code>. Search for <code>Remote-SSH: Settings</code> and then go to <code>Remote.SSH: Server Install Path</code>. Add Item as remote host <code>rackham.uppmax.uu.se</code> and Value as folder in which you want to install all your data and extensions <code>/proj/uppmax202x-x-xx/nobackup</code> (without a trailing slash <code>/</code>). </p> <p>If you already had your <code>vscode-server</code> running and storing extensions in home directory. Make sure to kill the server by selecting <code>Remote-SSH: KIll VS Code Server on Host</code> on Command Palette and deleting the <code>.vscode-server</code> directory in your home folder. </p>"},{"location":"VSCode/#22-install-extensions","title":"2.2. Install Extensions","text":"<p>You can sync all your local VSCode extensions to the remote server after you are connected with VSCode server on Rackham by searching for <code>Remote: Install Local Extensions in 'SSH: rackham.uppmax.uu.se'</code> in Command Palette. You can alternatively, go to Extensions tab and select each individually. </p>"},{"location":"VSCode/#23-selecting-kernels","title":"2.3. Selecting Kernels","text":"<p>Request allocation in either Rackham or Snowy compute node depending on your need, for that use <code>interactive</code> slurm command. Load the correct module on Rackham/Snowy that you contains the interpret you want on your VSCode. For example in case you need ML packages and python interpreter, do <code>module load python_ML_packages</code>. Check the file path for python interpreter by checking <code>which python</code> and copy this path. Go to Command Palette <code>Ctrl+Shift+P</code> or <code>F1</code> on your local VSCode. Search for \"interpreter\" for python, then paste the path of your interpreter/kernel. </p> <p><code>venv</code> or <code>conda</code> environments are also visible on VSCode when you select interpreter/kernel for python or jupyter server. For jupyter, you need to start the server first, check Point 3.</p>"},{"location":"VSCode/#3-working-with-jupyter-server-on-rackham-and-snowy","title":"3. Working with jupyter server on Rackham and snowy","text":""},{"location":"VSCode/#rackham","title":"Rackham:","text":"<p>Module load jupyter packages either from <code>module load python</code> or <code>module load python_ML_packages</code> as per your needs. For heavy compute and longer running jupyter server, allocate a Rackham compute node instead of using login node. Either request for rackham compute node by using, for example, <code>interactive -A uppmax202x-x-xx -p node -N 1 -t 2:00:00</code> or move to the next step to run jupyter on login node itself. Start the jupyter server <code>jupyter notebook --ip 0.0.0.0 --no-browser</code>. Copy the jupyter server URL which goes somethig like <code>http://r52.uppmax.uu.se:8888/tree?token=xxx</code>, click on Select Kernel on VSCode and select Existing Jupyter Server. Past the URL here and confirm your chioce. </p>"},{"location":"VSCode/#snowy","title":"Snowy:","text":"<p>Start an interactive session with GPU allocation on Snowy <code>interactive -A uppmax202x-x-xx -p node -N 1 -t 02:00:00 --gres=gpu:1 -M snowy</code>. Module load the jupyter packages <code>module load python_ML_packages</code> and start the jupyter server <code>jupyter notebook --ip 0.0.0.0 --no-browser</code>. This should start a jupyter server on Snowy compute node with one T4 GPU. Copy the URL of the running jupyter server which goes something like <code>http://s193.uppmax.uu.se:8888/tree?token=xxx</code> and paste it in the jupyter kernel path on your local VSCode. The application will automatically perform port forwarding to Rackham, which already is listening to Snowy compute nodes over certain ports.  </p>"},{"location":"commandline/","title":"The command-line on Bianca","text":"<p>Objectives</p> <ul> <li>We'll use the commands and investigate the Bianca environment</li> <li>Tips and tricks for Bianca users</li> </ul> <p>Warning</p> <ul> <li>We assume that you have already covered the Command-line material and tested on Rackham<ul> <li>LINUX</li> <li>Basic toolkit</li> </ul> </li> </ul>"},{"location":"commandline/#command-line-intro","title":"Command-line intro","text":""},{"location":"commandline/#navigation-and-file-management","title":"Navigation and file management","text":"<ol> <li><code>pwd</code>  \u2003 present directory</li> <li><code>ls</code>  \u2003list content</li> <li><code>cd</code>  \u2003change directory</li> <li><code>mkdir</code>  \u2003make directory</li> <li><code>cp</code>  \u2003copy</li> <li><code>scp</code>  \u2003securely remotely copy</li> <li><code>mv</code>  \u2003move</li> <li><code>rm</code>  \u2003remove</li> <li><code>rmdir</code>  \u2003remove empty directory</li> </ol>"},{"location":"commandline/#read-files-and-change-file-properties","title":"Read files and change file properties","text":"<ol> <li><code>cat</code>  \u2003print content on screen</li> <li><code>head</code>  \u2003print first part</li> <li><code>tail</code>  \u2003print last part</li> <li><code>less</code>  \u2003browse content</li> <li><code>tar</code>  \u2003compress or extract file</li> <li><code>chmod</code>  \u2003change file permissions</li> <li><code>man</code>  \u2003info about a command</li> </ol>"},{"location":"commandline/#type-along","title":"Type along","text":""},{"location":"commandline/#navigating-bianca","title":"Navigating Bianca","text":"<ul> <li>Check the path to your $HOME folder</li> </ul> <pre><code>$ cd ~\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/home/$USER\n/castor/project/home/bjornc\n</code></pre> <ul> <li>Check the path to your projects</li> </ul> <pre><code>$ cd /proj\n$ ls\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/proj\n/proj\n</code></pre> <pre><code>$ cd /sensXXX\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/proj/sensXXX\n/castor/project/proj\n</code></pre>"},{"location":"commandline/#aliases-on-your-desktop","title":"Aliases on your desktop","text":"<ol> <li>Be logged in with ThinLinc.</li> <li>Open a terminal.</li> <li>Run <code>cd Desktop\\</code></li> <li>Make shortcuts to your heart's content:</li> <li><code>PROJ=sens2023531</code></li> <li><code>ln -s /proj/$PROJ/ proj</code></li> <li><code>ln -s /proj/$PROJ/nobackup nobackup</code></li> <li><code>ln -s /proj/$PROJ/nobackup/wharf/&lt;yourusername&gt;/&lt;yourusername-$PROJ&gt; wharf</code></li> </ol> <p>You can also make aliases to executables, like the convenient <code>interactive</code>-job starting script in <code>proj/useful_files</code>.</p>"},{"location":"commandline/#the-terminal-and-the-gui-are-friends","title":"The terminal and the GUI are friends","text":"<p>On a clean terminal, try typing <code>cd</code> and then dragging a folder from the GUI to the terminal.</p> <p>It types the absolute path for you!</p>"},{"location":"commandline/#use-chmod-to-protect-important-files","title":"Use <code>chmod</code> to protect important files","text":"<p>Most projects rely on data that should in principle never be changed. Run <code>chmod -R -w data/</code> to remove write permissions on everything inside the <code>data</code> directory. </p> <p>Note that e.g. <code>rm --force</code> can still delete files, and you can overwrite or delete data by confirming when Bash prompts (if you own the files). However, this will protect you from most programmatic mistakes, the mistakes of others, and give you an opportunity to regret a typo.</p> <p>Some people use an alias for <code>rm</code> that causes it to always prompt for confirmation. I don't recommend this because it detracts from the effectiveness of write-protecting your own files.</p>"},{"location":"conda/","title":"Conda on Bianca","text":"<p>Objectives</p> <ul> <li>This is a brief description of the necessary steps to use the local Conda repository at UPPMAX, and install things for yourself or your project using Conda. </li> </ul> <p>TLDR;</p> <ul> <li><code>module load conda</code></li> <li><code>export CONDA_ENVS_PATH=/a/path/to/a/place/in/your/project/</code></li> <li><code>conda create</code> ... etc</li> <li>Remember to run <code>conda clean -a</code> once in a while. When you load the module, there is also a reminder displayed, so you get this info there also.</li> </ul>"},{"location":"conda/#conda-basics","title":"Conda basics","text":"<ul> <li>What does Conda do?</li> <li>How to create a Conda environment</li> </ul> <ul> <li>Learn pros and cons with Conda</li> <li>Learn how to install packages and work with the Conda (isolated)     environment</li> </ul> <p>Hint</p> <ul> <li>On Bianca Conda is the first choice when installing packages, because there is a local mirror of most of the Conda repositories.</li> </ul>"},{"location":"conda/#using-conda","title":"Using Conda","text":"<p>Conda cheat sheet</p> <ul> <li>List packages in present environment: <code>conda list</code></li> <li>List all environments: <code>conda info -e</code> or <code>conda env list</code></li> <li>Install a package: <code>conda install somepackage</code></li> <li>Install from certain channel (conda-forge):     <code>conda install -c conda-forge somepackage</code></li> <li>Install a specific version: <code>conda install somepackage=1.2.3</code></li> <li>Create a new environment: <code>conda create --name myenvironment</code></li> <li>Create a new environment from requirements.txt:     <code>conda create --name myenvironment --file requirements.txt</code></li> <li>On e.g. HPC systems where you don\u2019t have write access to central     installation directory: <code>conda create --prefix /some/path/to/env</code></li> <li>Activate a specific environment: <code>conda activate myenvironment</code></li> <li>Deactivate current environment: <code>conda deactivate</code></li> </ul>"},{"location":"conda/#installing-using-conda","title":"Installing using Conda","text":"<p>Note</p> <p>We have mirrored all major conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day. We have the following channels available:</p> <ul> <li>bioconda</li> <li>biocore</li> <li>conda-forge</li> <li>dranew</li> <li>free</li> <li>main</li> <li>pro</li> <li>qiime2</li> <li>r</li> <li>r2018.11</li> <li>scilifelab-lts</li> <li>nvidia</li> <li>pytorch</li> </ul> <p>You reach them all by loading the conda module. You don't have to state the specific channel when using UPPMAX. Also, you are offline on Bianca which means that the default is <code>--offline</code>, which you can specify if you want to simulate the experience on Rackham.</p> <p>If you need a channel that isn't in our repository, we can easily add it. Just send us a message and we will do it.</p>"},{"location":"conda/#first-steps","title":"First steps","text":"<p>Tip</p> <p>There will be an exercise in the end!</p> <ol> <li>First load our conda module (there is no need to install you own     miniconda, for instance)</li> </ol> <p>bash \\$</p> <p>module load conda</p> <ul> <li>This grants you access to the latest version of Conda and all     major repositories on all UPPMAX systems.</li> <li>Check the text output as conda is loaded, especially the first     time, see below</li> </ul> <p>Conda load output</p> <ul> <li>The variable CONDA_ENVS_PATH contains the location of your     environments. Set it to your project's environments folder if     you have one.</li> <li>Otherwise, the default is \\~/.conda/envs.</li> <li>You may run <code>source conda_init.sh</code> to initialise your shell to     be able to run <code>conda activate</code> and <code>conda deactivate</code> etc.</li> <li>Just remember that this command adds stuff to your shell outside     the scope of the module system.</li> <li>REMEMBER TO <code>conda clean -a</code> once in a while to remove unused     and unnecessary files</li> </ul> <ol> <li>First time</li> </ol> <ul> <li> <p>The variable CONDA_ENVS_PATH contains the location of your     environments. Set it to your project's environments folder if you     have one.</p> </li> <li> <p>Otherwise, the default is \\~/.conda/envs.</p> </li> <li> <p>Example:</p> <p>bash \\$</p> <p>export CONDA_ENVS_PATH=/proj/\\&lt;your-project-id&gt;/nobackup/\\&lt;username&gt;</p> </li> </ul> <p>By choice</p> <p>Run <code>source conda_init.sh</code> to initialise your shell (bash) to be able to run <code>conda activate</code> and <code>conda deactivate</code> etcetera instead of <code>source activate</code>. It will modify (append) your <code>.bashrc</code> file.</p> <ul> <li>When conda is loaded you will by default be in the base     environment, which works in the same way as other conda     environments. include a Python installation and some core system     libraries and dependencies of Conda. It is a \u201cbest practice\u201d to     avoid installing additional packages into your base software     environment.</li> </ul> <ol> <li>Create the conda environment</li> </ol> <ul> <li> <p>Example:</p> <p>bash \\$</p> <p>conda create --name python36-env python=3.6 numpy=1.13.1 matplotlib=2.2.2</p> <p>The <code>mamba</code> alternative</p> <ul> <li> <p><code>mamba</code> is a fast drop-in alternative to conda, using     \"libsolv\" for dependency resolution. It is available from the     <code>conda</code> module.</p> </li> <li> <p>Example:</p> <p>bash \\$</p> <p>mamba create --name python37-env python=3.7 numpy=1.13.1 matplotlib=2.2.2</p> </li> </ul> </li> </ul> <ol> <li> <p>Activate the conda environment by:</p> <p>bash \\$</p> <p>source activate python36-env</p> <ul> <li>You will see that your prompt is changing to start with     <code>(python-36-env)</code> to show that you are within an environment.</li> </ul> </li> <li> <p>Now do your work!</p> </li> <li> <p>Deactivate</p> </li> </ol> <p>conda deactivate</p> <p>Warning</p> <ul> <li>Conda is known to create many small files. Your diskspace is     not only limited in GB, but also in number of files (typically     <code>300000</code> in \\$home).</li> <li>Check your disk usage and quota limit with <code>uquota</code></li> <li>Do a <code>conda clean -a</code> once in a while to remove unused and     unnecessary files</li> </ul> <ul> <li>More info about Conda on     UPPMAX</li> </ul>"},{"location":"conda/#working-with-conda-environments-defined-by-files","title":"Working with Conda environments defined by files","text":"<ul> <li> <p>Create an environment based on dependencies given in an environment     file:</p> <pre><code>$ conda env create --file environment.yml\n</code></pre> </li> <li> <p>Create file from present conda environment:</p> <pre><code>$ conda env export &gt; environment.yml\n</code></pre> </li> </ul> <p><code>environments.yml</code> (for conda) is a yaml-file which looks like this:</p> <pre><code>name: my-environment\nchannels:\n- defaults\ndependencies:\n- numpy\n- matplotlib\n- pandas\n- scipy\n</code></pre> <p><code>environments.yml</code> with versions:</p> <pre><code>name: my-environment\nchannels:\n- defaults\ndependencies:\n- python=3.7\n- numpy=1.18.1\n- matplotlib=3.1.3\n- pandas=1.1.2\n- scipy=1.6.2\n</code></pre> <p>More on dependencies</p> <ul> <li>Dependency management from course Python for Scientific     computing</li> </ul>"},{"location":"conda/#exercises","title":"Exercises","text":"<p>UPPMAX: Create a conda environment and install some packages</p> <ul> <li>First check the current installed packages while having     <code>python/3.9.5</code> loaded</li> <li>Open a new terminal and have the old one available for later     comparison</li> <li>Use the conda module on Rackham and create an environment with name     <code>HPC-python23</code> with <code>python 3.7</code> and <code>numpy 1.15</code></li> </ul> <ul> <li> <p>Use your a path for <code>CONDA_ENVS_PATH</code> of your own choice or <code>/proj/py-r-jl/&lt;user&gt;/python</code></p> <p>:   -   (It may take a minute or so)</p> </li> </ul> <ul> <li>Activate!</li> <li>Check with <code>pip list</code> what is there. Compare with the environment     given from the python module in the first terminal window.</li> </ul> <ul> <li>Which version of Python did you get?</li> </ul> <ul> <li>Don't forget to deactivate the Conda environment before doing other     exercises!</li> </ul> <p>Solution for UPPMAX</p> <p>Write this in the terminal</p> <pre><code>$ module load conda\n$ export CONDA_ENVS_PATH=/proj/py-r-jl/&lt;user&gt;/python\n$ conda create --name HPC-python23 python=3.7 numpy=1.15\n$ source activate HPC-python23\n$ pip list\n$ python -V\n$ source deactivate\n</code></pre> <ul> <li> <p>Conda is an installer of packages but also bigger toolkits</p> </li> <li> <p>Conda creates isolated environments (see next section) not clashing     with other installations of python and other versions of packages</p> </li> <li> <p>Conda environment requires that you install all packages needed by     yourself.</p> <ul> <li>That is, you cannot load the python module and use the     packages therein inside you Conda environment.</li> </ul> </li> </ul> <p>keypoints</p> <ul> <li>No internet</li> <li>Local repositories</li> </ul>"},{"location":"containers/","title":"Containers","text":""},{"location":"containers/#containers","title":"Containers","text":""},{"location":"containers/#why","title":"Why?","text":"<p>Do you need a program that has a very complicated and exhausting install? Or maybe your installation requires root privileges?</p> <p>Within a container you can install whatever you want and send it to whichever computer you want to run it from. </p>"},{"location":"containers/#how","title":"How?","text":"<p>You can either download an existing container containing your software of interest or build one yourself. </p>"},{"location":"containers/#docker-vs-singularityapptainer","title":"Docker vs Singularity/Apptainer","text":"<p>Docker requires root privileges and therefore are run on local computers or the cloud.</p> <p>HPC-clusters without root privileges uses Apptainer. </p>"},{"location":"containers/#public-repositories","title":"Public repositories","text":"<p>Dockerhub is the most common and has the biggest repository: https://hub.docker.com/</p> <p>Apptainer/Singularity uses: https://cloud.sylabs.io/library Luckily, Apptainer can convert docker containers into Apptainer containers. </p> <p>Anyone can upload a container to the repositories. So be cautious and look for verified publishers.</p>"},{"location":"containers/#learn-more-about-containers","title":"Learn more about containers","text":"<p>Read more on the Uppmax singularity workshop: https://pmitev.github.io/UPPMAX-Singularity-workshop/</p>"},{"location":"containers/#example-i-want-gatk-on-bianca","title":"Example: I want gatk on bianca","text":"<p>Remember, no internet on bianca. Build on rackham and then transfer the container to the wharf. </p> <p>Can I find it on syslabs.io? https://cloud.sylabs.io/library/search?q=gatk No. Continue to Dockerhub.</p> <p>Dockerhub? https://hub.docker.com/r/broadinstitute/gatk/ Perfect.</p> <p><code>apptainer pull gatk_4.3.0.0.sif docker://broadinstitute/gatk:4.3.0.0</code></p> <p></p> <p><code>sftp</code> to the wharf:</p> <p></p> <p><code>apptainer exec gatk_4.3.0.0.sif gatk</code></p> <p></p>"},{"location":"containers/#set-the-apptainer-cache-dirs-to-projproj-id-to-prevent-quota-issues","title":"Set the Apptainer cache dirs to /proj/proj-id to prevent quota issues","text":"<pre><code>mkdir -p /proj/staff/bjornv/apptainer_cache_dir/{cache,tmp,localcache}\nexport APPTAINER_CACHEDIR=/proj/staff/bjornv/apptainer_cache_dir/cache\nexport APPTAINER_TMPDIR=/proj/staff/bjornv/apptainer_cache_dir/tmp\nexport APPTAINER_LOCALCACHEDIR=/proj/staff/bjornv/apptainer_cache_dir/localcache\n</code></pre>"},{"location":"exercises/","title":"Exercises","text":"<p>Info</p> <p>We put some exercises here for you, if you want to get some more hands-on.</p>"},{"location":"exercises/#working-with-modules","title":"Working with modules","text":"View in IGV <ul> <li> <p>Load the genome, the bam file, and the annotated vcf that we got from the demo into IGV for viewing</p> Answer <p>For this small example we use igv-core. Good is also to be on a compute node.</p> <p><pre><code>$ ml bioinfo-tools IGV\n$ igv-core --genome genome.fa \n</code></pre> Then open ERR1252289.subset.bam ERR1252289.subset.snpEff.vcf.gz from the GUI.</p> </li> </ul>"},{"location":"exercises/#transfering-files","title":"Transfering files","text":"Copy files between to Sens projects <ul> <li> <p>Use the Transit server to copy a file (e.g. the interactive session script) from the the Bianca workshop project to another project, if you belong to one. </p> Answer <ol> <li>Connect to transit</li> <li>Mount the projects with mount_wharf</li> <li>Move/copy the file(s) from sens2023531 to your other project.</li> <li>Log in to the other Sens project on Bianca and move the file from the wharf to a good place</li> </ol> </li> </ul>"},{"location":"exercises/#using-the-compute-nodes","title":"Using the compute nodes","text":"Submit a Slurm job <ul> <li>Make a batch job to run the demo \"Hands on: Processing a BAM file to a VCF using GATK, and annotating the variants with snpEff\". Ask for 2 cores for 1h.</li> </ul> Answer <ul> <li>edit a file using you prefered editor, named <code>my_bio_worksflow.sh</code>, for example, with the content</li> </ul> <pre><code>#!/bin/bash\n#SBATCH -A sens2023531\n#SBATCH -J workflow\n#SBATCH -t 01:00:00\n#SBATCH -p core\n#SBATCH -n 2\n\n\ncd /proj/sens2023531/workshop/slurm/\n\nmodule load bioinfo-tools\n\n# load samtools\nmodule load samtools/1.17\n\n# copy and example BAM file\ncp -a /proj/sens2023531/workshop/data/ERR1252289.subset.bam .\n\n# index the BAM file\nsamtools index ERR1252289.subset.bam\n\n# load the GATK module\nmodule load GATK/4.3.0.0\n\n# make symbolic links to the hg38 genomes\nln -s /sw/data/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.* .\n\n# create a VCF containing inferred variants\ngatk HaplotypeCaller --reference genome.fa --input ERR1252289.subset.bam --intervals chr1:100300000-100800000 --output ERR1252289.subset.vcf\n\n# use snpEFF to annotate variants\nmodule load snpEff/5.1\njava -jar $SNPEFF_ROOT/snpEff.jar eff hg38 ERR1252289.subset.vcf &gt; ERR1252289.subset.snpEff.vcf\n\n# compress the annotated VCF and index it\nbgzip ERR1252289.subset.snpEff.vcf\ntabix -p vcf ERR1252289.subset.snpEff.vcf.gz\n</code></pre> <ul> <li> <p>make the job script executable <pre><code>$ chmoad a+x my_bio_workflow.sh\n</code></pre></p> </li> <li> <p>submit the job <pre><code>$ sbatch my_bio_workflow.sh\n</code></pre></p> </li> </ul>"},{"location":"exercises/#doing-installations","title":"Doing installations","text":""},{"location":"exercises/#rpackage-installation","title":"Rpackage installation","text":"Install Tidycmprsk <ul> <li>Install the package Tidycmprsk on Rackham and use sftp to get it to Bianca.</li> <li>tidycmprsk on GitHub</li> </ul> Answer <p>https://uppmax.github.io/bianca_workshop/rpackages/#example-install-tidycmprsk</p>"},{"location":"exercises/#conda-installation","title":"Conda installation","text":"Install with Conda directly on Bianca <ul> <li>Install <code>python=3.7</code> and <code>numpy=1.15</code> with Conda directly on Bianca.</li> </ul> Answer <p>https://uppmax.github.io/bianca_workshop/conda/#exercises</p>"},{"location":"exercises/#pip-installation-with-virtual-environment","title":"Pip installation with virtual environment","text":"Install with pip <ul> <li>Make a virtual environment (confer this tutorial with python/3.8.7 on Rackham and install numpy==1.18.1 and matplotlib==3.1.3. Use sftp to get it to Bianca.</li> </ul> Answer <p>https://uppmax.github.io/bianca_workshop/pip</p>"},{"location":"exercises/#singularityapptainer","title":"Singularity/apptainer","text":"Install gatk on bianca with apptainer <ul> <li>Use the docker image for gatk/4.3.0.0 and insatll on rackham and tranfer to Bianca.</li> </ul> Answer <p>https://uppmax.github.io/bianca_workshop/containers/#example-i-want-gatk-on-bianca</p>"},{"location":"hardware_snowy/","title":"Snowy Hardware","text":"Nodes CPUs Cores Memory Scratch GPUs Name Comment 122 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 128GB 3/4TB N/A s1-s12, s14-s40, s42-s120, s201-s204 . 49 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 128GB 3/4TB Tesla T4 s151-s163, s164-s200 . 15 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 512GB 3/4TB N/A s121-s129, s131, s133-s137 <code>-C mem512GB</code> or <code>-C fat</code> 12 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 256GB 3/4TB N/A s139-s150 <code>-C mem256GB</code> or <code>-C fat</code> 1 2x Xeon E5-2660 2.2 GHz 80 (10 x 8) 4TB 3/4TB N/A s229 <code>-C mem4TB -p veryfat</code> 1 2x Xeon E5-2660 2.2 GHz 16 (2 x 8) 256GB 3/4TB Tesla T4 s138 <code>-C mem256GB</code> or <code>-C fat</code> <ul> <li>Job walltime : 30 days (720 hrs)  </li> <li>interactive walltime : 12 hrs  </li> <li><code>-p devel</code> or <code>-p devcore</code> walltime : 1 hr  </li> <li>No <code>-t</code> walltime : 1 hr  </li> <li>Total jobs allowed (running and waiting) : 5000  </li> <li>Path to node-local temp disk space (scratch folder) : <code>$SNIC_TMP</code></li> </ul> <p>GPU on Snowy: NVIDIA T4 (16GB)</p> <p>Performance in TFLOPS and TOPS (10<sup>12</sup> operations per sec):  </p> Data type A100 T4 FP64 9.7 | 19.5* 0.25 FP32 19.5 8.1 TF32 156** N/A FP16 312** 65 BF16 312** N/A Int8 624** 130 Int4 1248** 260 <p>*Performance on Tensor Core. **Up to a factor of two faster with sparsity.</p> <p>Submitting jobs : Slurm on snowy</p>"},{"location":"install/","title":"Software and package installation on Bianca","text":"<p>Objectives</p> <ul> <li>We'll go through the methods to install packages and tools</li> <li>We'll briefly get an overview of the hardware on Bianca</li> </ul>"},{"location":"install/#the-module-system","title":"The module system","text":"<ul> <li>As we have seen this morning, there is a lot of programs and tools installed as tools on Bianca.</li> <li>These have typically been installed on Rackham and is synced over to Bianca a couple of times per day.</li> <li>You can request installations but that may take several days or even weeks to be handled by the application experts at UPPMAX.</li> <li>But you may be able to do installations yourself. Here the use of Rackham comes handy because of the:<ul> <li>internet connection</li> <li>the computer architecture is somewhat similar such that precompiled binaries or compiled programs (x86_64) on Rackham will most often work also on Bianca.</li> <li>you can use the wharf to transfer source files and binaries to Bianca from Rackham</li> </ul> </li> </ul>"},{"location":"install/#install-software-yourself","title":"Install software yourself","text":"<ul> <li>You can install in your home directory.<ul> <li>This is handy for personal needs, low numbers of files (i.e. not Conda).</li> </ul> </li> <li>Usually better to install in project directory.<ul> <li>This way the project contains both data and software \u2014 good for reproducibility, collaboration, and everyone's general sanity.</li> </ul> </li> <li>If not available on Bianca already you may have to use the wharf to install your tools<ul> <li>alternatively let a Application Expert install the tool as a module.</li> </ul> </li> </ul>"},{"location":"install/#packages-and-libraries-to-scripting-programs","title":"Packages and libraries to scripting programs","text":"<ul> <li>Python, R and Julia all have some centrally installed packages that are available from the modules. </li> <li>R has a special module called <code>R_packages</code>, and some Machine Learning python packages are included in the <code>python_ml_packages</code> module.</li> <li>If not found there you can try to install those by yourself.</li> </ul> <p>Tip Python packages</p> <ul> <li>Try Conda first directly on Bianca. We have mirrored all major conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.</li> <li>If you want to keep number of files down, use PyPI (pip), but then you need to use Rackham and the wharf.</li> </ul>"},{"location":"install/#conda","title":"Conda","text":"<ul> <li> <p>We have mirrored all major conda repositories directly on UPPMAX, on both Rackham and Bianca. These are updated every third day.     We have the following channels available:</p> <ul> <li>bioconda</li> <li>biocore</li> <li>conda-forge</li> <li>dranew</li> <li>free</li> <li>main</li> <li>pro</li> <li>qiime2</li> <li>r</li> <li>r2018.11</li> <li>scilifelab-lts</li> </ul> </li> </ul> <p>More info</p> <ul> <li>Conda user guide</li> <li>https://uppmax.github.io/bianca_workshop/conda/</li> </ul>"},{"location":"install/#python-packages-with-pip","title":"Python packages with pip","text":"<p>Principle</p> <ul> <li>install on Rackham<ul> <li>pip install --user  <li>python setup.py install --user or --prefix= <li>sync to wharf</li> <li>move the files on Bianca</li> <li>you may have to update $PYTHONPATH</li> <p>More info</p> <ul> <li>https://uppmax.github.io/bianca_workshop/pip/</li> <li>Python packages</li> <li>https://uppmax.github.io/R-python-julia-HPC/python/packages.html</li> <li>https://uppmax.github.io/R-python-julia-HPC/python/isolated.html</li> </ul>"},{"location":"install/#r-packages","title":"R packages","text":"<ul> <li>On UPPMAX the module R_packages is a package library containing almost all packages in the</li> <li>CRAN and BioConductor repositories. </li> <li> <p>As of 2021-11-11 there are a total of 21659 R packages installed in R_packages/4.1.1. A total of 21740 packages are available in CRAN and BioConductor.</p> </li> <li> <p>You can quickly check if your package is there by:</p> </li> </ul> <p><code>$ ml R_packages/4.1.1</code></p> <p>Then <code>grep</code> for some package, in this case \"glmnet\".</p> <pre><code>$ ls -l $R_LIBS_SITE | grep glmnet\ndr-xr-sr-x  9 douglas sw  4096 Sep  6  2021 EBglmnet\ndr-xr-sr-x 11 douglas sw  4096 Nov 11  2021 glmnet\ndr-xr-sr-x  8 douglas sw  4096 Sep  7  2021 glmnetcr\ndr-xr-sr-x  7 douglas sw  4096 Sep  7  2021 glmnetUtils\n</code></pre> <p>More info</p> <ul> <li>https://uppmax.github.io/bianca_workshop/rpackages/</li> <li>https://uppmax.github.io/R-python-julia-HPC/R/packagesR.html</li> <li>https://uppmax.github.io/R-python-julia-HPC/R/isolatedR.html</li> </ul>"},{"location":"install/#julia-packages","title":"Julia packages","text":"<ul> <li>At UPPMAX there is a central library with installed packages.</li> <li>This is good, especially when working on Bianca, since you then do not need to install via the Wharf.</li> <li>A selection of the Julia packages and libraries installed on UPPMAX are:<pre><code>BenchmarkTools\nCSV\nCUDA\nMPI\nDistributed\nIJulia\nPlots\nPyPlot\nGadfly\nDataFrames\nDistributedArrays\nPlotlyJS\n</code></pre> </li> </ul> <p>More info</p> <ul> <li>https://uppmax.github.io/bianca_workshop/julia/</li> <li>https://uppmax.github.io/R-python-julia-HPC/julia/isolatedJulia.html</li> </ul>"},{"location":"install/#containers","title":"\"Containers\"","text":""},{"location":"install/#singularity","title":"Singularity","text":"<ul> <li>Singularity user guide</li> </ul>"},{"location":"install/#docker","title":"Docker","text":"<ul> <li>Docker will unfortunately not work on the clusters, since it requires root permission.</li> <li>However, Singularity may use Docker images.</li> </ul>"},{"location":"install/#build-from-source","title":"Build from source","text":"<ul> <li>We have several compiler versions from GNU and INTEL</li> <li>check with: <code>$ ml avail gcc</code> and <code>$ ml avail intel</code></li> <li>The safest way is to transfer the source code to Bianca via the wharf.</li> <li>Guide for compiling serial and parallel programs</li> <li>Available combinations of compilers and parallel libraries (openmpi): https://hackmd.io/_IqCbOiyS8SZ0Uqpa3UpHg?view</li> </ul>"},{"location":"install/#summary-about-the-bianca-hardware","title":"Summary about the Bianca Hardware","text":"<ul> <li>Intel Xeon E5-2630 v3 Huawei XH620 V3 nodes with 128, 256 or 512 GB memory</li> <li>GPU nodes withtwo NVIDIA A100 40GB GPUs each.</li> </ul> <p>Cores per node|16/64|</p> <p>Details about the compute nodes</p> <ul> <li>Thin nodes<ul> <li>204 compute nodes with single or dual CPUs and one 4TB mechanical drive or 1TB SSD</li> <li>Each CPU has 8 cores</li> </ul> </li> <li>Fat nodes<ul> <li>75 compute nodes, 256 GB memory each.</li> <li>15 compute nodes, 512 GB memory each</li> <li>10 compute nodes each equipped with 2xNVIDIA A100 (40GB) GPUs</li> </ul> </li> <li>Total number of CPU cores is 4800</li> <li>Login nodes have 2vCPU each and 16GB memory</li> <li>Network<ul> <li>Dual 10 Gigabit Ethernet for all nodes</li> </ul> </li> </ul> <p>Storage - Local disk (scratch): 4 TB  - Home storage: 32 GB at Castor - Project Storage: Castor</p> <p>Keypoints</p> <ul> <li>You have got an overview of the procedures to install packages/libraries and tools on Bianca through the wharf</li> <li>If you feel uncomfortable or think that many users would benefit from the software, ask the support to install it.</li> </ul>"},{"location":"intro/","title":"Overview","text":""},{"location":"intro/#what-is-needed-to-be-able-to-run-at-uppmax","title":"What is needed to be able to run at UPPMAX","text":"<ul> <li>SUPR<ul> <li>account</li> <li>project</li> </ul> </li> </ul>"},{"location":"intro/#how-to-access-the-clusters","title":"How to access the clusters?","text":"<ul> <li>login<ul> <li>ssh</li> <li>ThinLinc</li> </ul> </li> </ul>"},{"location":"intro/#where-should-you-keep-your-data","title":"Where should you keep your data?","text":"<ul> <li><code>uquota</code></li> </ul>"},{"location":"intro/#how-to-transfer-files","title":"How to transfer files?","text":"<ul> <li><code>sftp</code></li> <li><code>scp</code></li> </ul> <ul> <li>SFTP graphical tools  <ul> <li>WinSCP and FileZilla</li> </ul> </li> </ul>"},{"location":"intro/#the-module-system","title":"The module system","text":"<p>Built on LMOD, a module system to handle user's environment variables.</p> <p>Some useful comamnds:</p> <ul> <li><code>module avail &lt;name&gt;</code></li> <li><code>module spider &lt;name&gt;</code></li> <li><code>module load &lt;module&gt;/&lt;version&gt;</code></li> <li><code>module list</code></li> <li><code>module unload &lt;module&gt;/&lt;version&gt;</code></li> <li><code>module purge</code></li> </ul>"},{"location":"intro/#slurm","title":"Slurm","text":"<p>A job schedular used in many supercomputers and HPCs.  </p> <p>How to submit a job to Slurm?</p> <pre><code>sbatch  -A uppmax2025-3-5 -t 02:00:00  -p core  -n 10  my_job.sh\n</code></pre> <p>What should a jobscript contain?</p> <ul> <li><code>-A</code> : project number </li> <li><code>-t</code> : max time</li> <li><code>-p</code> : partition</li> <li><code>-n/-N</code> : number or core and/or nodes</li> <li><code>-J</code> : job name</li> <li>special features : <code>--gres, --gpus-per-node</code> etc.</li> </ul>"},{"location":"intro/#a-typical-job-script","title":"A typical job script:","text":"<pre><code>#!/bin/bash\n#SBATCH -A uppmax2025-3-5\n#SBATCH -p node\n#SBATCH -N 1\n#SBATCH -t 24:00:00\n\nmodule load software/version\n\n./my-script.sh\n</code></pre> <p>Useful SBATCH options:</p> <ul> <li><code>--mail-type=BEGIN,END,FAIL,TIME_LIMIT_80</code></li> <li><code>--output=slurm-%j.out</code></li> <li><code>--error=slurm-%j.err</code></li> </ul> <p>Useful commands:</p> <ul> <li><code>interactive -A naiss2023-22-247 -M snowy -p core -n 4</code> starts an interactive job on snowy</li> <li><code>jobinfo -p devel</code></li> <li><code>sinfo -p node -M snowy</code></li> <li><code>jobinfo -u username</code></li> </ul>"},{"location":"intro/#how-to-cancel-jobs","title":"How to cancel jobs?","text":"<ul> <li><code>scancel &lt;jobid&gt;</code></li> </ul>"},{"location":"intro/#job-dependencies","title":"Job dependencies","text":"<ul> <li><code>sbatch jobscript.sh</code>   submitted job with jobid1</li> <li><code>sbatch anotherjobscript.sh</code>  submitted job with jobid2</li> <li><code>--dependency=afterok:jobid1:jobid2</code> job will only start running after the successful end of jobs jobid1:jobid2</li> <li>very handy for clearly defined workflows</li> <li>One may also use <code>--dependency=afternotok:jobid</code> in case you\u2019d like to resubmit a failed job, OOM for example, to a node with a higher memory: <code>-C mem215GB</code> or <code>-C mem1TB</code> </li> <li>More in slurm documents.</li> </ul>"},{"location":"intro/#gpu-flags","title":"GPU flags","text":"<pre><code>#SBATCH -M snowy\n#SBATCH --gres=gpu:1\n</code></pre> Example of a job running on part of a GPU node <pre><code>#!/bin/bash\n#SBATCH -J GPUjob\n#SBATCH -A uppmax2025-3-5\n#SBATCH -t 03-00:00:00\n#SBATCH -p core\n#SBATCH -n 16\n#SBATCH -M snowy\n#SBATCH --gres=gpu:1\n\nmodule use /sw/EasyBuild/snowy/modules/all/\nmodule load intelcuda/2019b\n</code></pre> Example of an interactive session on Snowy <pre><code>interactive -A uppmax2025-3-5 -p node -N 1 -t 01:01:00 --gres=gpu:1\n</code></pre>"},{"location":"intro/#io-intensive-jobs-use-the-scratch-local-to-the-node","title":"I/O intensive jobs: use the scratch local to the node","text":"Example <pre><code>#!/bin/bash\n#SBATCH -J jobname\n#SBATCH -A uppmax2025-3-5\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 10:00:00\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17 samtools/1.14\n\nexport SRCDIR=$HOME/path-to-input\n\ncp $SRCDIR/foo.pl $SRCDIR/bar.txt $SNIC_TMP/.\ncd $SNIC_TMP\n\n./foo.pl bar.txt\n\ncp *.out $SRCDIR/path-to-output/.\n</code></pre>"},{"location":"intro/#job-arrays","title":"Job arrays","text":"Example <p>Submit many jobs at once with the same or similar parameters Use $SLURM_ARRAY_TASK_ID in the script in order to find the correct path</p> <pre><code>#!/bin/bash\n#SBATCH -A naiss2023-22-21\n#SBATCH -p node\n#SBATCH -N 2\n#SBATCH -t 01:00:00\n#SBATCH -J jobarray\n#SBATCH --array=0-19\n#SBATCH --mail-type=ALL,ARRAY_TASKS\n\n# SLURM_ARRAY_TASK_ID tells the script which iteration to run\necho $SLURM_ARRAY_TASK_ID\n\ncd /pathtomydirectory/dir_$SLURM_ARRAY_TASK_ID/\n\nsrun -n 40 my-program\nenv\n</code></pre> <p>You may use scontrol to modify some of the job arrays.</p>"},{"location":"intro/#gpu-accessibility-check","title":"GPU accessibility check","text":"<ul> <li> <p>Chech CUDA environment variable     <pre><code>[jayan@s180 uppmax2025-3-5]$ env | grep CUDA\nCUDA_VISIBLE_DEVICES=0\n</code></pre>     or     <pre><code>echo $CUDA_VISIBLE_DEVICES\n</code></pre></p> </li> <li> <p>Check CUDA and pytorch accessibility from python <pre><code>module load python_ML_packages\npython -c \"import torch; print(torch.__version__); print(torch.version.cuda); print(torch.cuda.get_device_properties(0)); print(torch.randn(1).cuda())\"\n</code></pre></p> </li> </ul>"},{"location":"intro/#additional-information","title":"Additional information","text":"<ul> <li>UPPMAX webpage</li> <li>Slurm</li> <li>Snowy</li> <li>GPU:s on Snowy</li> <li>Software and package installation</li> </ul>"},{"location":"isolated_envs/","title":"Use isolated environments","text":""},{"location":"isolated_envs/#isolated-environments","title":"Isolated environments","text":"<ul> <li>As an example, maybe you have been using TensorFlow 1.x.x for your project and <ul> <li>now you need to install a package that requires TensorFlow 2.x.x </li> <li>but you will still be needing the old version of TensorFlow for another package, for instance. </li> </ul> </li> <li> <p>This is easily solved with isolated environments.</p> </li> <li> <p>Another example is when a reviewer want you to remake a figure. </p> <ul> <li>You have already started to use a newer Python version or newer packages and </li> <li>realise that your earlier script does not work anymore. </li> </ul> </li> <li>Having freezed the environment would have solved you from this issue!</li> </ul> <p>Note</p> <p>Isolated/virtual environments solve a couple of problems:</p> <ul> <li>You can install specific, also older, package versions into them.</li> <li>You can create one for each project and no problem if the two projects require different versions.</li> <li>You can remove the environment and create a new one, if not needed or with errors.</li> <li>Good for reproducibility!</li> </ul> <ul> <li>Isolated environments let you create separate workspaces for different versions of Python and/or different versions of packages. </li> <li>You can activate and deactivate them one at a time, and work as if the other workspace does not exist.</li> </ul> <p>The tools</p> <ul> <li>Python's built-in <code>venv</code> module: uses pip       </li> <li><code>virtualenv</code> (can be installed): uses pip   </li> <li><code>conda</code>/<code>forge</code>: uses <code>conda</code>/<code>mamba</code> </li> </ul>"},{"location":"isolated_envs/#what-happens-at-activation","title":"What happens at activation?","text":"<ul> <li>Python version is defined by the environment.<ul> <li>Check with <code>which python</code>, should show at path to the environment.</li> <li>In conda you can define python version as well</li> <li>Since <code>venv</code> is part of Python you will get the python version used when running the <code>venv</code> command.</li> </ul> </li> <li>Packages are defined by the environent.<ul> <li>Check with <code>pip list</code></li> <li>Conda can only see what you installed for it.</li> <li>venv and virtualenv also see other packages if you allowed for that when creating the environment (<code>--system-site-packages</code>). </li> </ul> </li> <li>You can work in a Python shell or IDE (coming session)</li> <li>You can run scripts dependent on packages now instaleld in your environment.</li> </ul> <p>Tip</p> <ul> <li>Try with <code>venv</code> first</li> <li> <p>If very troublesome, try with <code>conda</code></p> </li> <li> <p>To use self-installed Python packages in a batch script, you also need to load the above mentioned modules and activate the environment. An example of this will follow later in the course. </p> </li> <li>To see which Python packages you, yourself, have installed, you can use <code>pip list --user</code> while the environment you have installed the packages in is active. To see all packages, use <code>pip list</code>. </li> </ul>"},{"location":"isolated_envs/#virtual-environment-venv-virtualenv","title":"Virtual environment - venv &amp; virtualenv","text":"<p>With this tool you can download and install with <code>pip</code> from the PyPI repository</p> venv vs. virtualenv <ul> <li>These are almost completely interchangeable</li> <li>The difference being that virtualenv supports older python versions and has a few more minor unique features, while venv is in the standard library.</li> <li>Step 1:<ul> <li>Virtualenv: <code>virtualenv --system-site-packages Example</code></li> <li>venv: <code>python -m venv --system-site-packages Example2</code></li> </ul> </li> <li>Next steps are identical and involves \"activating\" and <code>pip installs</code></li> <li>We recommend <code>venv</code> in the course. Then we are just needing the Python module itself!</li> </ul>"},{"location":"isolated_envs/#typical-workflow","title":"Typical workflow","text":"<ol> <li> <p>Start from a Python version you would like to use (load the module): </p> <ul> <li>This step are different at different clusters since the naming is different</li> </ul> </li> <li> <p>Load the Python module you will be using, as well as any site-installed package modules (requires the <code>--system-site-packages</code> option later)</p> <ul> <li><code>module load &lt;python module&gt;</code></li> </ul> </li> </ol> <p>The next points will be the same for all clusters</p> <ol> <li> <p>Create the isolated environment with something like <code>python -m venv &lt;name-of-environment&gt;</code> </p> <ul> <li>use the <code>--system-site-packages</code> to include all \"non-base\" packages</li> <li>include the full path in the name if you want the environment to be stored other than in the \"present working directory\".</li> </ul> </li> <li> <p>Activate the environment with <code>source &lt;path to virtual environment&gt;/bin activate</code></p> </li> </ol> <p>Note</p> <ul> <li><code>source</code> can most often be replaced by <code>.</code>, like in <code>. Example/bin/activate</code>. Note the important  after <code>.</code> <li>For clarity we use the <code>source</code> style here.</li> <ol> <li> <p>Install (or update) the environment with the packages you need with the <code>pip install</code> command</p> <ul> <li>Note that <code>--user</code> must be omitted: else the package will be installed in the global user folder.</li> <li>The <code>--no-cache-dir\"</code> option is required to avoid it from reusing earlier installations from the same user in a different environment. The <code>--no-build-isolation</code> is to make sure that it uses the loaded modules from the module system when building any Cython libraries.</li> </ul> </li> <li> <p>Work in the isolated environment</p> </li> <li>When activated you can always continue to add packages!</li> <li>Deactivate the environment after use with <code>deactivate</code></li> </ol> <p>Note</p> <p>To save space, you should load any other Python modules you will need that are system installed before installing your own packages! Remember to choose ones that are compatible with the Python version you picked!       <code>--system-site-packages</code> includes the packages already installed in the loaded python module.</p> <pre><code>$ module load python/3.11.5 \n$ python -m venv --system-site-packages Example_env\n</code></pre> <p>Warning</p> <p>Draw-backs</p> <ul> <li>Only works for Python environments</li> <li>Only works with Python versions already installed</li> </ul> Example <pre><code>ml python/3.11.5 \nwhich python\npython -V\ncd /proj/uppmax2025-3-5/private/&lt;username&gt;\npython -m venv torch_env\nsource activate  torch_env\npip install torch torchvision \npython\n</code></pre> <pre><code>&gt;&gt;&gt; import torch\n</code></pre> <p>Note</p> <ul> <li>You can use \"pip list\" on the command line (after loading the python module) to see which packages are available and which versions. </li> <li>Some packaegs may be inhereted from the moduels yopu have loaded</li> <li>You can do <code>pip list --local</code> to see what is installed by you in the environment.</li> <li>Some IDE:s like Spyder may only find those \"local\" packages</li> </ul>"},{"location":"isolated_envs/#conda","title":"Conda","text":"<ul> <li> <p>Conda is an installer of packages but also bigger toolkits and is useful also for R packages and C/C++ installations.</p> </li> <li> <p>Conda creates isolated environments not clashing with other installations of python and other versions of packages.</p> </li> <li>Conda environment requires that you install all packages needed by yourself. <ul> <li>That is,  you cannot load the python module and use the packages therein inside you Conda environment.</li> </ul> </li> </ul> Conda channels <ul> <li>conda-forge</li> <li>scilifelab-lts</li> <li>r</li> <li>main</li> <li>bioconda</li> <li>free</li> <li>pro</li> <li>qiime2</li> <li>biocore</li> <li>dranew</li> <li>r2018.11</li> <li>nvidia</li> <li>pytorch</li> <li> <p>anaconda</p> <p>You reach them all by loading the conda module. You don't have to state the specific channel when using UPPMAX. Otherwise you do with <code>conda -c &lt;channel&gt; ...</code></p> </li> </ul> <p>Warning</p> <p>Drawbacks</p> <ul> <li>Conda cannot use already install packages from the Python modules and libraries already installed, and hence installs them anyway</li> <li>Conda is therefore known for creating many small files. Your diskspace is not only limited in GB, but also in number of files (typically <code>300000</code> in $HOME). </li> <li>Check your disk usage and quota limit<ul> <li>Do a <code>conda clean -a</code> once in a while to remove unused and unnecessary files</li> </ul> </li> </ul> <p>Tip</p> <ul> <li>The conda environemnts includes many small files are by default stored in <code>~/.conda</code> folder that is in your $HOME directory with limited storage.</li> </ul> <p>This works nicely if you have several projects. Then you can change these varables according to what you are currently working with.</p> <pre><code>export CONDA_ENVS_PATH=\"path/to/your/project/(subdir)\"\nexport CONDA_PKG_DIRS=\"path/to/your/project/(subdir)\"\n</code></pre>"},{"location":"isolated_envs/#typical-workflow_1","title":"Typical workflow","text":"<p>The first 2 steps are cluster dependent and will therefore be slightly different.</p> <ol> <li> <p>Make conda available from a software module: <code>source /sw/apps/conda/latest/rackham_stage/etc/profile.d/conda.sh</code>, or use own installation of miniconda or miniforge.</p> </li> <li> <p>First time</p> </li> </ol> First time <ul> <li>The variables CONDA_ENVS_PATH and CONDA_PKG_DIRS contains the location of your environments. Set it to your project's environments folder, if you have one, instead of the $HOME folder.</li> <li>Otherwise, the default is <code>~/.conda/envs</code>. </li> <li>Example:</li> </ul> <pre><code>$ export CONDA_ENVS_PATH=\"path/to/your/project/(subdir)\"\n$ mkdir -p $CONDA_ENVS_PATH\n$ export CONDA_PKG_DIRS=\"path/to/your/project/(subdir)\"\n$ mkdir -p $CONDA_PKGS_DIRS\n</code></pre> <ul> <li>Disable the auto activation of the <code>(base)</code> environment <code>(base) conda config --set auto_activate_base false</code></li> <li>Logout and login again!</li> </ul> <p>Next steps are the same for all clusters</p> <ol> <li>Create the conda environment <code>conda create -n &lt;name-of-env&gt;</code></li> <li> <p>Activate the conda environment by: <code>source activate &lt;conda-env-name&gt;</code></p> <ul> <li>You can define the packages to be installed here already.</li> <li>If you want another Python version, you have to define it here, like: <code>conda ... python=3.6.8</code></li> </ul> </li> <li> <p>Install the packages with <code>conda install ...</code> or <code>pip install ...</code></p> </li> <li> <p>Now do your work!</p> <ul> <li>When activated you can always continue to add packages!</li> </ul> </li> <li> <p>Deactivate</p> </li> </ol> <pre><code>(python-36-env) $ conda deactivate\n</code></pre> Comments <ul> <li>When pinning with Conda, use single <code>=</code> instead of double (as used by pip)</li> </ul> Conda base env <ul> <li>When conda is loaded you will by default be in the base environment, which works in the same way as other conda environments. It includes a Python installation and some core system libraries and dependencies of Conda. It is a \"best practice\" to avoid installing additional packages into your base software environment.</li> </ul> Conda cheat sheet <ul> <li>List packages in present environment: <code>conda list</code></li> <li>List all environments:            <code>conda info -e</code> or <code>conda env list</code></li> <li>Install a package: <code>conda install somepackage</code></li> <li>Install from certain channel (conda-forge): <code>conda install -c conda-forge somepackage</code></li> <li>Install a specific version: <code>conda install somepackage=1.2.3</code></li> <li>Create a new environment: <code>conda create --name myenvironment</code></li> <li>Create a new environment from requirements.txt: <code>conda create --name myenvironment --file requirements.txt</code></li> <li>On e.g. HPC systems where you don't have write access to central installation directory: conda create --prefix /some/path/to/env`</li> <li>Activate a specific environment: <code>conda activate myenvironment</code></li> <li>Deactivate current environment: <code>conda deactivate</code></li> </ul> Conda vs mamba etc... <ul> <li>what-is-the-difference-with-conda-mamba-poetry-pip</li> </ul> What to do when a problem arises? <ul> <li>If you experience unexpected problems with the conda provided by the module system on Rackham or anaconda3 on Dardel, you can easily install your own and maintain it yourself.</li> <li>Read more at Pavlin Mitev's page about conda on Rackham/Dardel and change paths to relevant one for your system.</li> <li>Or Conda - \"best practices\" - UPPMAX</li> </ul>"},{"location":"isolated_envs/#install-from-file","title":"Install from file","text":""},{"location":"isolated_envs/#venv","title":"venv","text":"<p><code>pip install -r requirements.txt</code></p>"},{"location":"isolated_envs/#conda_1","title":"conda","text":"<p><code>conda env create -f environment.yaml</code></p>"},{"location":"isolated_envs/#exercises","title":"Exercises","text":"<p>Exercise 0: Make a decision between <code>venv</code> or <code>conda</code>.</p> <ul> <li>go with <code>venv</code> first as it is simpler to setup.</li> <li>go with <code>conda</code> if you need many envs to manage.</li> </ul> <p>Exercise 1: Cover the documentation for venvs or conda</p> <p>First try to find it by navigating.</p> <p>For venv: - Python venv documentation at UPPMAX - Video Tutorial By Richel</p> <p>For conda: - Installing using available source file - Your own conda installation - Installing using available module</p> <p>Exercise 2: Prepare the course environment</p> <p>install transformers torch torchvision notebook</p>"},{"location":"julia/","title":"Using Julia packages on Bianca","text":"<p>Warning</p> <p>under construction</p>"},{"location":"julia/#uppmax-central-library","title":"UPPMAX Central library","text":"<p>Info</p> <p>The Julia application at UPPMAX comes with several preinstalled packages. A selection of the Julia packages and libraries installed on UPPMAX are:</p> <ul> <li>BenchmarkTools</li> <li>CSV</li> <li>CUDA</li> <li>MPI</li> <li>Distributed</li> <li>IJulia</li> <li>Plots</li> <li>PyPlot</li> <li>Gadfly</li> <li>DataFrames</li> <li>DistributedArrays</li> <li>PlotlyJS</li> </ul> <ul> <li>You may control the present \"central library\" by typing <code>ml help julia/&lt;version&gt;</code> in the BASH shell.</li> <li>A possibly more up-to-date status can be found from the Julia shell:</li> </ul> <pre><code>    using Pkg\n    Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.8\");     #change version (1.8) accordingly if you have another main version of Julia\n    Pkg.status()\n    Pkg.activate(DEPOT_PATH[1]*\"/environments/v1.8\");     #to return to user library\n</code></pre>"},{"location":"julia/#first-time-run-on-bianca","title":"First time run on Bianca","text":"<p>You may have to build a package the first time with Pkg.build(\u201c\u201d). Since \u201c is pre-installed centrally on UPPMAX you must activate the central environment by following these steps below. This should only be needed the first time like this <pre><code>&gt; using Pkg\n&gt; Pkg.activate(DEPOT_PATH[2]*\"/environments/v1.8\");\n&gt; Pkg.build(\"&lt;package&gt;\")\n</code></pre>"},{"location":"julia/#install-yourself","title":"Install yourself","text":"<p>If you have started Julia once you will get the folders like this in the ~/.julia folder.</p> <pre><code>   $ tree .julia/ -d -L 1\n   .\n   \u251c\u2500\u2500 artifacts\n   \u251c\u2500\u2500 bin\n   \u251c\u2500\u2500 compiled\n   \u251c\u2500\u2500 conda\n   \u251c\u2500\u2500 environments\n   \u251c\u2500\u2500 logs\n   \u251c\u2500\u2500 packages\n   \u251c\u2500\u2500 prefs\n   \u251c\u2500\u2500 registries\n   \u2514\u2500\u2500 scratchspaces\n</code></pre> <p>The plan is that what you install on Rackham should be moved here in the same manner</p> <ul> <li>Make an installation of the package on Rackham in the Julia package manager</li> <li>Use a transfer method to move the package files to the wharf<ul> <li>To be certain to include all files, you may transfer the whole .julia dir. However, that can grow rather big with time.</li> </ul> </li> </ul> <p>Transfer to the Wharf</p> <p><pre><code>sftp douglas-sens2017625@bianca-sftp\nsftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <ul> <li>Alt1: If you would like all yor locally installed packages:</li> </ul> <pre><code>sftp&gt; put -r ~/.julia\n</code></pre> <ul> <li> <p>Alt 2: Just transfer the latest installed python package(s)</p> </li> <li> <p>Check what was installed. It may have been several dependency packages as well. Look at the times!</p> </li> </ul> <pre><code>sftp&gt;  lls -lrt ~/.julia/packages\n</code></pre> <pre><code>sftp&gt; put -r ~/.julia/packages/&lt;package name 1&gt;\n# and if several packages\nsftp&gt; put -r ~/.julia/packages/&lt;package name 2&gt;\n# and so on...\n</code></pre> <p>Move to site-packages folder On Bianca</p> <pre><code>cd /proj/sens2023531/nobackup/wharf/bjornc/bjornc-sens2023531/\nmv \u2013a  &lt;file(s)&gt; ~/.julia/packages/\n</code></pre> <p>If many files or packages</p> <p>you may want to tar before copying to include all possible symbolic links:</p> <p><pre><code>$ tar cfz &lt;tarfile.tar.gz&gt; &lt;package&gt;\n</code></pre> and in target directory (wharf_mnt) on Bianca:</p> <pre><code>$ tar xfz &lt;tarfile.tar.gz&gt; #if there is a tar file!\n$ mv \u2013a  &lt;file(s)&gt; .julia/packages/\n</code></pre>"},{"location":"jupyter/","title":"Running Jupyter on Snowy","text":"<p>Info</p> <ul> <li>You can run Python in <code>jupyter-notebook</code> or <code>jupyter-lab</code>, which provide a web interface with possibility of inline figures and debugging.</li> <li><code>jupyter-lab</code> is installed in the <code>python</code> modules <code>&gt;=3.10.8</code>.</li> <li>You may also install your own version of <code>jupyter-notebook</code> or <code>jupyter-lab</code> if you have certain version requirements.</li> </ul>"},{"location":"jupyter/#1-connect-to-rackham-using-thinlinc","title":"1. Connect to Rackham using ThinLinc.","text":"<p>You may download the ThinLinc client from here: https://www.cendio.com/thinlinc/download</p> <p>You may also use the web browser to connect: https://rackham-gui.uppmax.uu.se You need to set up 2FA for the ThinLinc web.</p>"},{"location":"jupyter/#2-open-a-terminal-in-thinlinc-and-ask-for-an-interactive-session-to-snowy","title":"2. Open a terminal in ThinLinc and ask for an interactive session to Snowy.","text":"<pre><code>interactive -A uppmax2025-3-5 -p node -N 1 -M snowy --gpus=1 -t 04:00:00\n</code></pre> <p>Note: We have a magnetic 10-node reservation for today: <code>uppmax2025-3-5</code>.</p> <p>Is the GPU visible? <pre><code>echo $CUDA_VISIBLE_DEVICES\n</code></pre></p>"},{"location":"jupyter/#3-load-the-needed-modules","title":"3. Load the needed modules","text":"<pre><code>module load python_ML_packages/3.9.5-gpu\npip list\n</code></pre> <p>And start the Jupyter notebook: <pre><code>jupyter-notebook --ip 0.0.0.0 --no-browser\n</code></pre></p>"},{"location":"jupyter/#4-open-the-notebook","title":"4. Open the notebook.","text":""},{"location":"jupyter/#solution-1","title":"Solution 1:","text":"<p>Click on any of the links. This will open a Jupyter notebook in a Firefox browser on Rackham.</p>"},{"location":"jupyter/#solution-2","title":"Solution 2:","text":"<p>Instead of opening Jupyter in a browser in ThinLinc, you may use the browser on your computer. You need to use ssh-tunnelling for this:</p> <pre><code>ssh -L 8000:s175:8888 iusan@rackham.uppmax.uu.se\n</code></pre> <p>If you cannot open the notebook using the links above, try changing it to: <code>localhost:&lt;port&gt;/?token&lt;tokenID&gt;</code> Example: <code>localhost:8000/?token=ab8a98df7b6606316849015daebaca5d2fa6331d32a47446</code></p>"},{"location":"linux/","title":"Command-line intro","text":"<p>Objectives</p> <ul> <li>We'll use the commands and investigate the Bianca environment</li> </ul> <p>Warning</p> <ul> <li>We assume that you have already covered the Command-line material and tested on Rackham<ul> <li>LINUX</li> <li>Basic toolkit</li> </ul> </li> </ul> <p>### Navigation and file management</p> <ol> <li><code>pwd</code>  \u2003 present directory</li> <li><code>ls</code>  \u2003list content</li> <li><code>cd</code>  \u2003change directory</li> <li><code>mkdir</code>  \u2003make directory</li> <li><code>cp</code>  \u2003copy</li> <li><code>scp</code>  \u2003securely remotely copy</li> <li><code>mv</code>  \u2003move</li> <li><code>rm</code>  \u2003remove</li> <li><code>rmdir</code>  \u2003remove empty directory</li> </ol>"},{"location":"linux/#read-files-and-change-file-properties","title":"Read files and change file properties","text":"<ol> <li><code>cat</code>  \u2003print content on screen</li> <li><code>head</code>  \u2003print first part</li> <li><code>tail</code>  \u2003print last part</li> <li><code>less</code>  \u2003browse content</li> <li><code>tar</code>  \u2003compress or extract file</li> <li><code>chmod</code>  \u2003change file permissions</li> <li><code>man</code>  \u2003info about a command</li> </ol>"},{"location":"linux/#type-along","title":"Type along","text":""},{"location":"linux/#navigating-bianca","title":"Navigating Bianca","text":"<ul> <li>Check the path to your $HOME folder</li> </ul> <pre><code>$ cd ~\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/home/$USER\n/castor/project/home/bjornc\n</code></pre> <ul> <li>Check the path to your projects</li> </ul> <pre><code>$ cd /proj\n$ ls\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/proj\n/proj\n</code></pre> <pre><code>$ cd /sensXXX\n$ pwd\n$ pwd -P\n</code></pre> Answer <pre><code>/proj/sensXXX\n/castor/project/proj\n</code></pre>"},{"location":"login/","title":"Log in to Bianca","text":"<p>Objectives</p> <ul> <li>We'll go through the methods to log in</li> </ul> <p>Note</p> <ul> <li>Remember that Rackham is your friend as well in your work. </li> <li>Being able to work there as well will improve your possibilities to work effectively when you also need some sort of internet connection. For instance:<ul> <li>installing tools</li> <li>installing Python, R and Julia packages</li> <li>transfer scripts</li> <li>updating your git repositories (not containing sensitive data)</li> </ul> </li> </ul>"},{"location":"login/#biancas-design","title":"Bianca's design","text":"<ul> <li>Bianca was designed<ul> <li>to make accidental data leaks difficult</li> <li>to make correct data management as easy as possible</li> <li>to emulate the HPC cluster environment that SNIC users were familiar with</li> <li>to provide a maximum amount of resources</li> <li>and to satisfy regulations.</li> </ul> </li> </ul>"},{"location":"login/#bianca-has-no-internet","title":"Bianca has no Internet","text":"<pre><code>- Still you can log in, but it is done in two steps!\n- We recomend the ThinLink web portal, to enable graphics\n</code></pre> <ul> <li>Bianca is only accessible from within Sunet (i.e. from university networks).</li> <li>Use VPN outside Sunet. Link to VPN for UU</li> <li>You can get VPN credentials from all Swedish universities.</li> </ul> <ul> <li>The whole Bianca cluster (blue) contains hundreds of virtual project clusters (green), each of which is isolated from each other and the Internet.</li> <li>Data can be transferred to or from a virtual project cluster through the Wharf, which is a special file area that is visible from the Internet.</li> </ul>"},{"location":"login/#log-in","title":"Log in","text":"<ul> <li>You can log in either through ThinLinc or via ssh</li> <li>If you are using graphics of an kind, use ThinLinc</li> <li>Otherwise, if you just ned the commandline, it is enough to use ssh.<ul> <li>ssh from home terminal</li> <li>ssh from a session on Rackham </li> </ul> </li> </ul>"},{"location":"login/#log-in-to-bianca-with-thinlinc","title":"Log in to Bianca with ThinLinc","text":"<ul> <li>Bianca offers graphical login<ul> <li>You need to be on SUNET or use VPN</li> <li>On web:<ul> <li>https://bianca.uppmax.uu.se</li> <li>requires 2-factor authentication</li> </ul> </li> </ul> </li> </ul> <p>Warning</p> <p>The ThinLinc application will work for Rackham but not Bianca</p> <p></p>"},{"location":"login/#the-log-in-steps","title":"The log in steps","text":"<ol> <li>When you log in to https://bianca.uppmax.uu.se, your SSH or ThinLinc client first meets the blue Bianca login node.<ul> <li>user name: <code>&lt;username&gt;-&lt;projid&gt;@bianca.uppmax.uu.se</code><ul> <li>like: <code>myname-sens2023531@bianca.uppmax.uu.se</code></li> </ul> </li> <li>password: your password, directly followed by the 6-digit 2-factor<ul> <li>like: verysecret678123</li> </ul> </li> </ul> </li> <li>After checking your [2-factor authentication] this server looks for your virtual project cluster.</li> <li>If it's present, then you are transferred to a login prompt on your cluster's login node. If not, then the virtual cluster is started.<ul> <li>you are prompted to give your username and password again, this time without projid and 2nd-factor:<ul> <li>username: <code>&lt;myname&gt;</code></li> <li>password: verysecret</li> </ul> </li> </ul> </li> <li>Inside each virtual project cluster, by default there is just a one-core login node. When you need more memory or more CPU power, you submit a job (interactive or batch), and an idle node will be moved into your project cluster.</li> </ol>"},{"location":"login/#log-in-via-ssh","title":"Log in via ssh","text":"<p>You may try to log in any of your terminals https://uppmax.github.io/uppmax_intro/login2.html#terminals</p> <p><pre><code>    $ ssh &lt;user&gt;-sens2023531@bianca.uppmax.uu.se\n</code></pre> - you are prompted to give your password directly followed by the 6-digit 2-factor         - like: verysecret678123</p>"},{"location":"login/#start-an-interactive-session","title":"Start an interactive session","text":"<p>To be able to work with the type alongs we strongly recommend you to start an interactive session already now.</p> <ul> <li>More about interactive sessions and Slurm in the afternoon, but we don't need a further insight in this to proceed now!</li> </ul> <p>Start the interactive session</p> <p>We start an 8 hour session with 2 cores.</p> <pre><code>$ interactive -A sens2023531 -p core -n 2 -t 8:0:0\n</code></pre> <p>keypoints</p> <ul> <li>We recommend you to use ThinLinc to log in when you need graphics</li> </ul>"},{"location":"modules1/","title":"Working with the modules on Bianca","text":"<p>Objectives</p> <ul> <li>Using module system commands</li> <li>Use software in the module system for some typical workflows</li> </ul> <ul> <li>800+ programs and packages are installed, with usually many versions of each.</li> <li>To avoid chaos and collisions, they are managed by a module system called LMOD.</li> <li>By default, the only software available on Bianca and other UPPMAX clusters are standard Linux tools.</li> <li>All other software is made available by using the module system.</li> <li>This means users must explicitly request software, together with software versions, that they require.</li> <li>This turns out to be quite easy to do. It also improves readability and reproducibility.</li> <li>Nearly all software in the module system is available on all UPPMAX clusters.</li> </ul> <p>Warning</p> <ul> <li>To access bioinformatics tools, load the bioinfo-tools module first.</li> </ul>"},{"location":"modules1/#modules","title":"Modules","text":"<ul> <li>Software at UPPMAX</li> <li>Module system</li> </ul>"},{"location":"modules1/#some-commands","title":"Some commands","text":"<p>The <code>module</code> command is the basic interface to the module system. The <code>ml</code> shortcut command is also available.</p> <ul> <li>list all modules immediately available, or search for a specific available module<ul> <li><code>module avail</code> or <code>ml av</code></li> <li><code>module avail *tool*</code> or <code>ml av *tool*</code></li> </ul> </li> </ul> <p>This command is not so smart, though, especially when searching for a specific tool, or a bioinformatics tool. It only reports modules that are immediately available.</p> <p><pre><code>$ ml av R\n</code></pre> outputs everything that has an <code>r</code> in the name... not useful.</p> <pre><code>$ ml av samtools\nNo module(s) or extension(s) found!\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre> <p>It is better to use <code>module spider</code> or <code>ml spider</code>. If there is an exact match, it reports it first.</p> <pre><code>$ ml spider R\n\n-------------------------------------------\n  R:\n-------------------------------------------\n     Versions:\n        R/3.0.2\n        R/3.2.3\n        R/3.3.2\n        R/3.4.0\n        R/3.4.3\n        R/3.5.0\n        R/3.5.2\n        R/3.6.0\n        R/3.6.1\n        R/4.0.0\n        R/4.0.4\n        R/4.1.1\n        R/4.2.1\n     Other possible modules matches:\n        454-dataprocessing  ADMIXTURE  ANTLR  ARCS  ARC_assembler  ARPACK-NG  ART  AdapterRemoval  AlienTrimmer  Amber  AnchorWave  Arlequin  Armadillo  ArrowGrid  Bamsurgeon  BclConverter  BioBakery  BioBakery_data  ...\n\n-------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*R.*'\n\n-------------------------------------------\n  For detailed information about a specific \"R\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider R/4.2.1\n-------------------------------------------\n</code></pre> <pre><code>$ ml spider samtools\n\n-------------------------------------------\n  samtools:\n-------------------------------------------\n     Versions:\n        samtools/0.1.12-10\n        samtools/0.1.19\n        samtools/1.1\n        samtools/1.2\n        samtools/1.3\n        samtools/1.4\n        samtools/1.5_debug\n        samtools/1.5\n        samtools/1.6\n        samtools/1.8\n        samtools/1.9\n        samtools/1.10\n        samtools/1.12\n        samtools/1.14\n        samtools/1.16\n        samtools/1.17\n     Other possible modules matches:\n        SAMtools\n\n-------------------------------------------\n  To find other possible module matches execute:\n\n      $ module -r spider '.*samtools.*'\n\n-------------------------------------------\n  For detailed information about a specific \"samtools\" package (including how to load the modules) use the module's full name.\n  Note that names that have a trailing (E) are extensions provided by other modules.\n  For example:\n\n     $ module spider samtools/1.17\n-------------------------------------------\n</code></pre> <p>The final bit of output tells us more about a specific module version, including the special step required to access all bioinformatics modules.</p> <pre><code>$ ml spider samtools/1.17\n\n-------------------------------------------\n  samtools: samtools/1.17\n-------------------------------------------\n\n    You will need to load all module(s) on any one of the lines below before the \"samtools/1.17\" module is available to load.\n\n      bioinfo-tools\n\n    Help:\n        samtools - use samtools 1.17\n\n        Version 1.17\n</code></pre> <p>This reminds us that we need to load the <code>bioinfo-tools</code> module to be able to load <code>samtools/1.17</code>. Again, this is required (just once) before loading bioinformatics software.</p> <ul> <li>Load a module <ul> <li><code>module load &lt;module name&gt;</code> or <code>ml &lt;module name&gt;</code></li> </ul> </li> </ul> <p>When loading a module, there is a \"default\" module available, which is almost always the latest version. However, we rarely want to rely on that. For reproducibility, we want to load specific version of our bioinformatics tools. To load the <code>samtools/1.17</code> module, which is a bioinformatics module.</p> <pre><code>$ ml bioinfo-tools\n$ ml samtools/1.17\n</code></pre> <ul> <li>List the loaded modules<ul> <li><code>module list</code> or simply <code>ml</code></li> </ul> </li> </ul> <pre><code>$ ml\n\nCurrently Loaded Modules:\n  1) uppmax   2) bioinfo-tools   3) samtools/1.17\n</code></pre> <p>To load <code>GATK/4.3.0.0</code> now, <code>bioinfo-tools</code> is not required because it is already loaded. Loading this module also shows that sometimes, loading a module results in a message that is helpful for using the module at UPPMAX.</p> <pre><code>$ ml GATK/4.3.0.0\nNote that all versions of GATK starting with 4.0.8.0 use a different wrapper\nscript (gatk) than previous versions of GATK.  You might need to update your\njobs accordingly.\n\nThe complete GATK resource bundle is in /sw/data/GATK\n\nSee 'module help GATK/4.3.0.0' for information on activating the GATK conda\nenvironment for using DetermineGermlineContigPloidy and similar other tools.\n</code></pre> <p>This message references the command <code>module help GATK/4.3.0.0</code> for additional help with this module. All modules have at least a brief help message. Some (such as GATK/4.3.0.0) have more extensive help that guides users using features of the modules at UPPMAX. It is not general help for using the tool itself.</p> <ul> <li>Display a brief module-specific help.<ul> <li><code>module help &lt;module name&gt;</code> or <code>ml help &lt;module name&gt;</code> </li> </ul> </li> </ul> <pre><code>$ ml help GATK/4.3.0.0\n\n-------------- Module Specific Help for \"GATK/4.3.0.0\" ---------------\nGATK - use GATK 4.3.0.0\nVersion 4.3.0.0\n\n**GATK 4.3.0.0**\n\nUsage:\n\n    gatk --help     for general options, including how to pass java options\n\n    gatk --list     to list available tools\n\n    gatk ToolName -OPTION1 value1 -OPTION2 value2 ...\n                  to run a specific tool, e.g., HaplotypeCaller, GenotypeGVCFs, ...\n\nFor more help getting started, see\n\n    https://software.broadinstitute.org/gatk/documentation/article.php?id=9881\n\n...\n</code></pre> <p>When we list the modules loaded with <code>ml</code>, we see that <code>GATK/4.3.0.0</code> is now loaded, as is its prerequisite module <code>java/sun_jdk1.8.0_151</code>.</p> <pre><code>$ ml\n\nCurrently Loaded Modules:\n  1) uppmax   2) bioinfo-tools   3) samtools/1.17   4) java/sun_jdk1.8.0_151   5) GATK/4.3.0.0\n</code></pre> <p>Modules can also be unloaded, which also unloads their prerequisites.</p> <ul> <li>Unload a module <ul> <li><code>module unload &lt;module name&gt;</code> or <code>ml -&lt;module name&gt;</code></li> </ul> </li> </ul>"},{"location":"modules1/#installed-software","title":"Installed software","text":"<p>You can find almost all installed software at:</p> <ul> <li>https://www.uppmax.uu.se/resources/software/installed-software/</li> </ul>"},{"location":"modules1/#installed-databases","title":"Installed databases","text":"<p>You can find descriptions of almost all installed databases at:</p> <ul> <li>Installed databases at UPPMAX</li> </ul>"},{"location":"modules1/#workflows","title":"Workflows","text":"Hands on: Processing a BAM file to a VCF using GATK, and annotating the variants with snpEff <p>This workflow uses a pre-made BAM file that contains a subset of reads from a sample from European Nucleotide Archive project PRJEB6463 aligned to human genome build hg38. These reads are from the region <code>chr1:100300000-100800000</code>.</p> <ol> <li> <p>Copy example BAM file to your working directory. <pre><code>$ cp -a /proj/sens2023531/workshop/data/ERR1252289.subset.bam .\n</code></pre></p> </li> <li> <p>Take a quick look at the BAM file. First see if <code>samtools</code> is available. <pre><code>$ which samtools\n</code></pre></p> </li> <li> <p>If <code>samtools</code> is not found, load <code>bioinfo-tools</code> then <code>samtools/1.17</code> <pre><code>$ ml bioinfo-tools samtools/1.17\n</code></pre></p> </li> <li> <p>Now create an index for the BAM file, and examine the first 10 reads aligned within the BAM file. <pre><code>$ samtools index ERR1252289.subset.bam\n$ samtools view ERR1252289.subset.bam | head\n</code></pre></p> </li> <li> <p>Looks good. Now load the <code>GATK/4.3.0.0</code> module. <pre><code>$ module load GATK/4.3.0.0\n</code></pre></p> </li> <li> <p>Make symbolic links to hg38 genome resources already available on UPPMAX. This provides local symbolic links for the hg38 resources <code>genome.fa</code>, <code>genome.fa.fai</code> and <code>genome.dict</code>. <pre><code>$ ln -s /sw/data/iGenomes/Homo_sapiens/UCSC/hg38/Sequence/WholeGenomeFasta/genome.* .\n</code></pre></p> </li> <li> <p>Create a VCF containing inferred variants. Speed it up by confining the analysis to this region of chr1. <pre><code>$ gatk HaplotypeCaller --reference genome.fa --input ERR1252289.subset.bam --intervals chr1:100300000-100800000 --output ERR1252289.subset.vcf\n</code></pre> This produces as its output the files <code>ERR1252289.subset.vcf</code> and <code>ERR1252289.subset.vcf.idx</code>.</p> </li> <li> <p>Now use <code>snpEff/5.1</code> to annotate the variants. Loading <code>snpEff/5.1</code> results in a change of java prerequisite. Also take a quick look at the help for the module for help with running this tool at UPPMAX. <pre><code>$ ml snpEff/5.1\n\nThe following have been reloaded with a version change:\n  1) java/sun_jdk1.8.0_151 =&gt; java/OpenJDK_12+32\n\n$ ml help snpEff/5.1\n\n------------------- Module Specific Help for \"snpEff/5.1\" --------------------\n    snpEff - use snpEff 5.1\n    Version 5.1\n\n    Usage: java -jar $SNPEFF_ROOT/snpEff.jar ...\n\n    Usage: java -jar $SNPEFF_ROOT/SnpSift.jar ...\n    along with the desired command and possible java options for memory, etc\n\n    Note that databases must be added by an admin -- request via support@uppmax.uu.se\n    See http://snpeff.sourceforge.net/protocol.html for general help\n\nEvery database that is provided by snpEff/5.1 as of this installation is installed.  This complete list\ncan be generated with\n\n    java -jar $SNPEFF_ROOT/snpEff.jar databases\n\nThree additional databases have been installed.\n\n    Database name                  Description                                      Notes\n    -------------                  -----------                                      -----\n    c_elegans.PRJNA13758.WS283     Caenorhabditis elegans genome version WS283      MtDNA uses Invertebrate_Mitochondrial codon table\n    canFam4.0                      Canis familiaris genome version 4.0\n    fAlb15.e73                     Ficedula albicollis ENSEMBLE 73 release\n\nThe complete list of locally installed databases is available at $SNPEFF_ROOT/data/databases_list.installed\n\nTo add your own snpEff database, see the guide at http://pcingola.github.io/SnpEff/se_buildingdb/#option-1-building-a-database-from-gtf-files\n</code></pre></p> </li> <li> <p>Annotate the variants. <pre><code>$ java -jar $SNPEFF_ROOT/snpEff.jar eff hg38 ERR1252289.subset.vcf &gt; ERR1252289.subset.snpEff.vcf\n</code></pre></p> </li> <li> <p>Take a quick look! <pre><code>$ less ERR1252289.subset.snpEff.vcf\n</code></pre></p> </li> <li> <p>Compress the annotated VCF and index it, using <code>bgzip</code> and <code>tabix</code> provided by the <code>samtools/1.17</code> module, already loaded. <pre><code>$ bgzip ERR1252289.subset.snpEff.vcf\n$ tabix -p vcf ERR1252289.subset.snpEff.vcf.gz\n</code></pre></p> </li> </ol> Hands on: Running R within RStudio, use ggplot2 from R_packages/4.1.1 <ol> <li> <p>Load the <code>R_packages/4.1.1</code> module and the latest <code>RStudio</code> module, and start RStudio with <code>rstudio &amp;</code>. </p> </li> <li> <p>Load the <code>ggplot2</code> R library, provided by <code>R_packages/4.1.1</code>, and produce an example plot. </p> </li> <li> <p>Save the plot using <code>ggsave</code>. </p> </li> </ol> Hands on: Loading the conda/latest module <ol> <li>Load the <code>conda/latest</code> module. <pre><code>$ ml conda/latest\nThe variable CONDA_ENVS_PATH contains the location of your environments. Set it to your project's environments folder if you have one.\nOtherwise, the default is ~/.conda/envs. Remember to export the variable with export CONDA_ENVS_PATH=/proj/...\n\nYou may run \"source conda_init.sh\" to initialise your shell to be able\nto run \"conda activate\" and \"conda deactivate\" etc.\nJust remember that this command adds stuff to your shell outside the scope of the module system.\n\nREMEMBER TO USE 'conda clean -a' once in a while\n</code></pre></li> </ol> <p>We want to set the <code>CONDA_ENVS_PATH</code> variable to a directory within our project, rather than use the default which is our home directory. If you do not set this variable, your home directory will easily exceed its quotas when creating even a single conda environment. This will be covered in more detail in the afternoon.</p> <p>keypoints</p> <ul> <li>Use the module system to use centrally installed software that is available on all nodes. </li> <li>Include versions when loading modules, for reproducibility!</li> <li>Your own installed software, scripts, python packages etc. are available from their paths.</li> </ul>"},{"location":"naiss-sens-bianca/","title":"NAISS-SENS, sensitive data and Bianca","text":"<p>Objectives</p> <ul> <li>A brief overview of various aspects of sensitive data projects</li> <li>A discussion of NAISS SENS projects, proposals, and allocations</li> <li>... and an overview of the Bianca system</li> </ul>"},{"location":"naiss-sens-bianca/#sensitive-personal-data","title":"Sensitive personal data","text":"<ul> <li> <p>Personal data: Traced to now living persons, e.g.</p> <ul> <li>Name</li> <li>Address</li> <li>Food preference</li> <li>Size of left nostril</li> </ul> </li> <li> <p>Sensitive personal data:</p> <ul> <li>ethnic origin</li> <li>political opinion</li> <li>religious or philosophical beliefs</li> <li>trade union membership</li> <li>health</li> <li>sex life</li> <li>genetic data</li> <li>biometric data that can uniquely identify a person<ul> <li>including some image or auditory recordings</li> </ul> </li> </ul> </li> <li> <p>More about sensitive personal data</p> <ul> <li>IMY</li> <li>GDPR</li> <li>Data protection</li> <li>Skydd av personuppgifter</li> <li>SND</li> </ul> </li> <li> <p>When in doubt, contact your university's data protection officer.</p> </li> <li> <p>Other sensitive data:</p> <ul> <li>Confidential information<ul> <li>e.g. IP from private industry</li> </ul> </li> <li>Secrets<ul> <li>sensitive environmental data, e.g. protected species      </li> </ul> </li> <li>National security</li> </ul> </li> <li> <p>A Data Protection Impact Assessment(DPIA) is legal requirement for any project with GDPR-data.</p> </li> <li> <p>If in doubt (especially with \"new\" data type), also do a information security evaluation to determine how you should handle the data.</p> </li> </ul>"},{"location":"naiss-sens-bianca/#pseudonymisation-and-anonymisation","title":"Pseudonymisation and anonymisation","text":"<ul> <li>Anonymised: no theoretical way at all to determine an individual</li> <li>Pseudonymised: anything less</li> </ul> <p>Pseudonymisation is a security mechanism that improves the security of sensitive personal data. The data is still sensitive.</p> <p>But how do you know whether data is anonymised?      - \"It depends...\"      - Some data cannot be anonymised at all (e.g. whole genome sequence)     - One metric for microdata is K-anonymity</p>"},{"location":"naiss-sens-bianca/#apply-for-project","title":"Apply for project","text":"<ul> <li>Detailed instructions for project application Open NAISS SENS Rounds</li> <li>Before GDPR-data will be transferred to UPPMAX, there must be a Data Processing Agreement between UU and the data controlling organisation. These are currently specific to the PI (and sometimes project).</li> </ul> <p>Some definitions</p> <ul> <li>Project: A collection of resource allocations and people, with an expiration date.</li> <li>Compute resources: CPU time used by submitted jobs.</li> <li>Storage resources: GB of disk space. /proj has backup, /proj/no-backup has no backup.</li> <li>Allocation: The amount of resources a project may consume before hitting a limit.</li> </ul> <ul> <li>Take home message: follow the above instructions, be as complete as detailed as you reasonably can. Submit requests early in the month. </li> </ul>"},{"location":"naiss-sens-bianca/#bianca","title":"Bianca","text":"<ul> <li>Bianca is a great platform for computationally intensive research on sensitive personal data. It can also be useful for:<ul> <li>national and international collaboration on sensitive personal data (without a high compute need)</li> <li>other types of sensitive data</li> <li>making sensitive data accessible (on Bianca)<ul> <li>Swegen</li> <li>SIMPLER</li> </ul> </li> </ul> </li> <li>Bianca is not intended for:<ul> <li>storing (inactive) data</li> </ul> </li> </ul>"},{"location":"naiss-sens-bianca/#biancas-design","title":"Bianca's design","text":"<ul> <li>Bianca was designed<ul> <li>for sensitive data from large-scale molecular experiments<ul> <li>but has since grown into new domains</li> </ul> </li> <li>to make accidental data leaks difficult</li> <li>to make correct data management as easy as possible</li> <li>to emulate the HPC cluster environment that SNIC/NAISS users were familiar with</li> <li>to provide a maximum amount of resources</li> <li>and to satisfy regulations.</li> </ul> </li> </ul> <p>Some definitions</p> <p>\u2014 Node: A basic \"computer\", with processor, RAM memory, local disk, and network connection. - Core: A part of a processor (CPU), capable of executing a thread of execution. - Thread: A series of logical steps, executing a program. - Multithreading: A program that runs with many threads in parallel. Each thread can occupy one core.</p>"},{"location":"naiss-sens-bianca/#bianca-has-no-internet","title":"Bianca has no Internet","text":"<p>... but we have \u201csolutions\u201d</p> <p></p> <ul> <li>Bianca is only accessible from within Sunet (i.e. from university networks).</li> <li>Use VPN outside Sunet. Link to VPN for UU<ul> <li>You can get VPN credentials from all Swedish universities.</li> </ul> </li> </ul> <p></p> <ul> <li>The whole Bianca cluster (blue) contains hundreds of virtual project clusters (green), each of which is isolated from each other and the Internet.</li> <li>Data can be transferred to or from a virtual project cluster through the Wharf, which is a special file area that is visible from the Internet.</li> </ul>"},{"location":"naiss-sens-bianca/#common-questions","title":"Common Questions","text":"<p>What should I do if I have both sensitive and non-sensitive data? - It may be convenient to have a separate project on Rackham. Scripts and pipelines can then be built on Rackham and moved to Bianca. - Having non-sensitive data on Bianca is okay, if maintaining two separate projects is impractical.</p> <p>One project or many? When should I apply for another project?  - Because every member in a project should be assumed to have full access to all data, each different constellation of collaborators needs its own project. - Because of the extra effort required to move data between projects, NAISS SENS projects can be extended and a continuation proposal is not necessary. - Let data, ethical consent, and practicality rule.</p> <p>I need more core-hours! - Do you really?      - First, use jobstats to determine whether you've been using your allocation efficiently.     - Second, remember that you can still submit and run jobs after your allocation is out. Such \"bonus\" jobs run after normal-priority jobs. Typically, they will run in the evening, within a couple of days. - If you know that you've been submitting efficient jobs and the wait time in the queue is an actual problem, then contact UPPMAX support and request more time. Motivate your request.</p> <p>I need more storage space! - Do you really?     - First, make an inventory of the data in your project \u2014 what do you have, how much space does it take, and why is it there?     - Second, delete data that you don't have an immediate plan to analyse.     - Also convert all .sam files to .bam and compress all your .fastq files. - If you've done all this and you still need space, contact UPPMAX support and request more space. Motivate your request by summarising your inventory. Include an estimate of your future needs. </p> <p>Keypoints</p> <ul> <li>Sensitive Personal data is data that could identify a person and that have implication</li> <li>The workflow for a project is When doing your Data management plan, do a DPIA, apply for PUBA(if apropriate), apply for project, DO scicens , Transfer resulted data, close project.</li> </ul>"},{"location":"overview/","title":"Overview","text":"<p>Objectives</p> <ul> <li>We'll get an overview of UPPMAX and SNIC/NAISS and how a computer cluster works</li> </ul> <p>UPPMAX = UppMACS - Uppsala Multidisciplinary Center for Advanced Computational Science</p>"},{"location":"overview/#naiss","title":"NAISS","text":"<ul> <li>National Academic Infrastructure for Supercomputing in Sweden</li> <li> <p>Mission: to provide a quality high-performance computing environment nationally</p> </li> <li> <p>Starting 1 January 2023, the National Academic Infrastructure for Supercomputing in Sweden (NAISS) is the new organization for high-performance computing, storage, and date services for academic users in Sweden. </p> </li> <li>From the users perspective, there will initially only be minimal differences between the SNIC and NAISS regimes.<ul> <li>FAQ:s \u2014 https://www.naiss.se/</li> </ul> </li> <li>Application rounds: https://www.naiss.se//#application-rounds-for-compute-and-storage-resources</li> <li>NAISS and Uppsala University fund UPPMAX \u2014 UU\u2019s supercomputing center.</li> </ul>"},{"location":"overview/#uppmax-missions","title":"UPPMAX missions","text":"<ul> <li>Runs the clusters placed in Uppsala.</li> <li>More details in the afternoon about Organisational orienteering!</li> </ul>"},{"location":"overview/#uppmax-systems","title":"UPPMAX systems","text":"<ul> <li>Clusters<ul> <li>Rackham (general purpose)<ul> <li>Snowy (Long runs and GPU:s)</li> </ul> </li> <li>Bianca (sensitive data)<ul> <li>Miarka (new for LifeScience)</li> </ul> </li> </ul> </li> <li>Storage<ul> <li>On-load directly connected to the clusters</li> <li>Off-load for large data not needed for computation analysis anymore</li> </ul> </li> <li>Cloud<ul> <li>Dis (region EAST-1)</li> </ul> </li> </ul>"},{"location":"overview/#high-performance-computing-hpc","title":"High Performance Computing \u2014 HPC","text":""},{"location":"overview/#what-is-a-cluster","title":"What is a cluster?","text":"<ul> <li> <p>A network of computers, each computer working as a node.</p> </li> <li> <p>From small scale RaspberryPi cluster... </p> </li> </ul> <p></p> <ul> <li>To supercomputers like Rackham.</li> </ul> <p></p> <ul> <li>Each node contains several processor cores and RAM and a local disk called scratch.</li> </ul> <p></p> <ul> <li> <p>The user logs in to login nodes  via Internet through ssh or Thinlinc.</p> </li> <li> <p>Here the file management and lighter data analysis can be performed.</p> </li> </ul> <p></p> <p></p> <ul> <li>The calculation nodes have to be used for intense computing. </li> </ul>"},{"location":"overview/#summary-about-the-three-common-uppmax-clusters","title":"Summary about the three \"common\" UPPMAX clusters","text":"Rackham Snowy Bianca Purpose General-purpose General-purpose Sensitive #  Nodes (Intel) 486+144 228+ 50 Nvidia T4 GPUs 288 +  10 nodes \u00e1 2 NVIDIA A100 GPUs Cores per node 20/16 16 16/64 Memory per node 128 GB 128 GB 128 GB Fat nodes 256 GB &amp; 1 TB 256, 512 GB &amp; 4 TB 256 &amp; 512 GB Local disk (scratch) 2/3 TB 4 TB 4 TB Login nodes Yes No (reached from Rackham) Yes (2 cores and 15 GB) \"Home\" storage Domus Domus Castor \"Project\" Storage Crex, Lutra Crex, Lutra Castor"},{"location":"overview/#overview-of-the-uppmax-systems","title":"Overview of the UPPMAX systems","text":"<pre><code>\n  graph TB\n\n  Node1 -- interactive --&gt; SubGraph2Flow\n  Node1 -- sbatch --&gt; SubGraph2Flow\n  subgraph \"Snowy\"\n  SubGraph2Flow(calculation nodes) \n        end\n\n        thinlinc -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        terminal -- usr --&gt; Node1\n        terminal -- usr-sensXXX + 2FA + VPN ----&gt; SubGraph1Flow\n        Node1 -- usr-sensXXX + 2FA + no VPN ----&gt; SubGraph1Flow\n\n        subgraph \"Bianca\"\n        SubGraph1Flow(Bianca login) -- usr+passwd --&gt; private(private cluster)\n        private -- interactive --&gt; calcB(calculation nodes)\n        private -- sbatch --&gt; calcB\n        end\n\n        subgraph \"Rackham\"\n        Node1[Login] -- interactive --&gt; Node2[calculation nodes]\n        Node1 -- sbatch --&gt; Node2\n        end</code></pre> <p>Next session</p> <p>We will try the different forms to log in to Bianca!</p> <p>keypoints</p> <ul> <li>NAISS makes available large-scale high-performance computing resources, storage capacity, and advanced user support, for Swedish research. </li> <li>UPPMAX runs the local resources placed at Uppsala Universty</li> <li>A cluster consists of several inter-connected computers that can work individually or together.</li> </ul>"},{"location":"pip/","title":"Install with pip to Bianca","text":""},{"location":"pip/#check-for-packages","title":"Check for packages","text":"<ul> <li>from the Python shell with the <code>import</code> command</li> <li> <p>from BASH shell with the </p> </li> <li> <p><code>pip list</code> command </p> </li> <li><code>ml help python/3.9.5</code> at UPPMAX</li> </ul> <p>Is it not there? Then proceed!</p> <p>Info</p> <p>Methods:</p> <ul> <li>You can either just download a python package, transfer to Wharf and Bianca and install there.</li> <li>Install it on Rackham. Perhaps you need it here as well! Then transfer to Wharf and Bianca local python library.</li> <li>Make a virtual environment with one or several packages on Rackham. Then transfer to Wharf and Bianca (any place).</li> </ul>"},{"location":"pip/#just-download-on-rackham-and-install-on-bianca","title":"Just download on Rackham and install on Bianca","text":"<p>Rackham <pre><code>$ pip download &lt;package-name&gt;\n</code></pre></p> <p>Transfer to the wharf</p> <p><pre><code>sftp douglas-sens2017625@bianca-sftp\nsftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <p>Now, upload to the wharf the package  and all the dependency packages <code>pip download</code> got you. <pre><code>sftp&gt; put -r &lt;package-name&gt;\n</code></pre> <p>Install on Bianca</p> <p>On Bianca install it (Yes, you can do it from this place) by telling pip where to look for packages and dependencies</p> <p><pre><code>$ ml python\n$ pip install --user --no-index --find-links &lt;path-to-packages&gt; &lt;package-name&gt;\n</code></pre>  is where your packages are, if in present working directory it is <code>.</code> <p>Then the package ends up in <code>~/.local/lib/python&lt;version&gt;/site-packages/</code> .</p>"},{"location":"pip/#install-on-rackham-and-then-tranfer-to-bianca","title":"Install on Rackham and then tranfer to Bianca","text":"<p>Info</p> <p>The package ends up on Rackham in <code>~/.local/lib/python&lt;version&gt;/site-packages/</code> .</p> <ul> <li>Note that python is omitting the last number (bug fix), like 3.8 for python-3.8.7. <p>Install on Rackham</p> <p><pre><code>$ ml python/&lt;version&gt;       # this is to make use the correct python version and possible dependencies already available\n$ pip install --user &lt;package-name&gt;\n</code></pre> - If there is a requirements.txt file with the content of packages to be installed:</p> <pre><code>pip install --user -r requirements.txt\n</code></pre> <p>Then the package(s) ends up in <code>~/.local/lib/python&lt;version&gt;/site-packages/</code> .</p> <p>Transfer to the Wharf</p> <p><pre><code>sftp douglas-sens2017625@bianca-sftp\nsftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <ul> <li>Alt1: If you would like all yor locally installed packages:</li> </ul> <pre><code>sftp&gt; put -r ~/.local/lib/python&lt;version&gt;/site-packages/\n</code></pre> <ul> <li> <p>Alt 2: Just transfer the latest installed python package(s)</p> </li> <li> <p>Check what was installed. It may have been several dependency packages as well. Look at the times!</p> </li> </ul> <pre><code>sftp&gt;  lls -lrt ~/.local/lib/python&lt;version&gt;/site-packages/\n</code></pre> <pre><code>sftp&gt; put -r ~/.local/lib/python&lt;version&gt;/site-packages/&lt;package name 1&gt;\n# and if several packages\nsftp&gt; put -r ~/.local/lib/python&lt;version&gt;/site-packages/&lt;package name 2&gt;\n# and so on...\n</code></pre> <p>Move to site-packages folder On Bianca</p> <pre><code>cd /proj/sens2023531/nobackup/wharf/bjornc/bjornc-sens2023531/\nmv \u2013a  &lt;file(s)&gt; ~/.local/lib/python&lt;version&gt;/site-packages/\n</code></pre> <p>If many files or packages</p> <p>you may want to tar before copying to include all possible symbolic links:</p> <p><pre><code>$ tar cfz &lt;tarfile.tar.gz&gt; &lt;package&gt;\n</code></pre> and in target directory (wharf_mnt) on Bianca:</p> <pre><code>$ tar xfz &lt;tarfile.tar.gz&gt; #if there is a tar file!\n$ mv \u2013a  &lt;file(s)&gt; ~/.local/lib/python&lt;version&gt;/site-packages/\n</code></pre>"},{"location":"pip/#isolatedvirtual-environments","title":"Isolated/virtual environments","text":"<ul> <li>We HIGHLY recommend using a virtual environment during installation, since this makes it easier to install for different versions of Python.  </li> </ul> <p>Note</p> <p>Isolated environments solve a couple of problems:</p> <ul> <li>You can install specific package, also older, versions into them.</li> <li>You can create one for each project and no problem if the two projects require different versions.</li> <li>You can remove the environment and create a new one, if not needed or with errors.</li> </ul> <ul> <li>More information here. </li> </ul> <p>Example, where python packages from the loaded module are used (<code>--system-site-packages</code>)</p> <pre><code>$ module load python/3.6.8\n$ python -m venv --system-site-packages &lt;path&gt;/projectB\n</code></pre> <p>\u201cprojectB\u201d is the name of the virtual environment. The directory \u201cprojectB\u201d is created in the present working directory. The <code>-m</code> flag makes sure that you use the libraries from the python version you are using.   </p> <ul> <li>Activate and install with pip (package one by one or from requirements.txt)</li> </ul> <p><pre><code>$ source &lt;path&gt;/projectB/bin/activate\n</code></pre> - Note that your prompt is changing to start with (analysis) to show that you are within an environment. - Install the packages from the file::</p> <pre><code>$ pip install -r requirements.txt\n\n$ pip list   # check\n$ deactivate\n</code></pre> <ul> <li>Virtual environments can be saved easily anywhere</li> </ul> <p>Transfer to the Wharf</p> <p><pre><code>sftp douglas-sens2017625@bianca-sftp\nsftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <pre><code>sftp&gt; put -r &lt;path&gt;/projectB\n</code></pre> <p>Move to site-packages folder On Bianca</p> <pre><code>cd /proj/sens2023531/nobackup/wharf/bjornc/bjornc-sens2023531/\nmv \u2013a  projectB &lt;path to any place, like project folder&gt;\n</code></pre> <p></p> <p>Error</p> <p>If problems arise, send an email to support@uppmax.uu.se and we'll help you.</p>"},{"location":"practicalities/","title":"Practicalities","text":""},{"location":"practicalities/#prerequisities","title":"Prerequisities","text":"<ul> <li>SUPR/course project</li> <li> <p>set up 2FA</p> <ul> <li>2-factor authentication</li> </ul> </li> <li> <p>Use VPN outside Sunet. Link to VPN for UU</p> </li> <li> <p>You can get VPN credentials from all Swedish universities.</p> </li> <li> <p>1st day of the intro course, 1/2-1 day at your own pace, beneficial to do before the workshop</p> <ul> <li>Linux</li> <li>Basic toolkit</li> </ul> </li> </ul>"},{"location":"practicalities/#_1","title":"Practicalities","text":"<p>Suggestion for window layout</p> <p>If you have just one screen (e.g. a laptop), we recommend arranging your windows like this:</p> <pre><code>\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   WEB     \u2551 \u2551  TERMINAL   \u2551\n\u2551  BROWSER  \u2551 \u2551   WINDOW    \u2551\n\u2551  WINDOW   \u2551 \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\u2551   WITH    \u2551 \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551   THE     \u2551 \u2551   BROWSER   \u2551\n\u2551  STREAM   \u2551 \u2551   W/HACKMD  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n</code></pre>"},{"location":"practicalities/#questions","title":"Questions","text":"<p>Most questions here, please</p> <ul> <li>HackMD: https://hackmd.io/@bclaremar/Bianca-workshop-March23?both</li> <li>This is archived and can be used as part of the course material afterwards</li> <li>Type in the left frame. <ul> <li>Don't bother about the formatting if you are not acquainted with markdown language!</li> </ul> </li> </ul> <p>Relevant for the presenting teacher</p> <ul> <li>Zoom chat</li> </ul>"},{"location":"practicalities/#schedule","title":"Schedule","text":"<ul> <li> <p>Morning session</p> <ul> <li>Login</li> <li>Command-line</li> <li>modules</li> <li>transfering files</li> </ul> </li> <li> <p>Afternoon session</p> <ul> <li>NAISS-SENS and sensitive data</li> <li>Slurm</li> <li>Software and package installation</li> <li>Q/A with extra material<ul> <li>Extra material on demand  </li> </ul> </li> </ul> </li> </ul> <p>Note</p> <p>It is OK to take parts of the course only!</p>"},{"location":"practicalities/#preliminary-schedule","title":"Preliminary schedule","text":"Topic Start Timing Content Syllabus and overview 9.00 15 intro Login 9.15 45 Login ssh/ThinLinc, 2FA Coffee break 10.00 15 Break Working with the command-line 10.15 30 Command-line intro Module system 10.45 25 Module system Short break 11.10 5 Break Transferring files 11.15 45 Transfering files to/from Bianca LUNCH break 12.00 60 NAISS-SENS and sensitive data 13.00 60 NAISS-SENS and sensitive data Short break 14.00 5 Break Compute nodes and slurm 14.05 20 Compute nodes and slurm Software installation 14.25 15 Software installation Summary 14.40 10 Summary Coffee break 14.50 15 Break Q/A 15.05 55 Time for interaction"},{"location":"profiling/","title":"Profiling","text":""},{"location":"profiling/#cpu","title":"CPU","text":""},{"location":"profiling/#jobstats","title":"jobstats","text":"<p><pre><code>jobstats --plot [jobid]\n</code></pre> Read more : https://docs.uppmax.uu.se/software/jobstats/</p>"},{"location":"profiling/#htop","title":"htop","text":""},{"location":"profiling/#projplot","title":"projplot","text":"<p><pre><code>projplot -A [project_code]\n</code></pre> Read more : https://docs.uppmax.uu.se/software/projplot/</p>"},{"location":"profiling/#gpu","title":"GPU","text":""},{"location":"profiling/#nvidia-smi","title":"nvidia-smi","text":"<pre><code>nvidia-smi dmon -o DT\nnvidia-smi --format=noheader,csv --query-compute-apps=timestamp,gpu_name,pid,name,used_memory --loop=1 -f sample_run.log\n</code></pre>"},{"location":"profiling/#nvtop","title":"nvtop","text":"<p>image source</p> <pre><code>module load nvtop\nnvtop\n</code></pre>"},{"location":"profiling/#pytorch","title":"Pytorch","text":""},{"location":"profiling/#memory_viz","title":"memory_viz","text":"<p>CUDA OOMs memory snapshot and memory profiler  </p> <p></p> <p>Start: <code>torch.cuda.memory._record_memory_history(max_entries=100000)</code> Save: <code>torch.cuda.memory._dump_snapshot(f\"{file_name}.pickle\")</code> Stop: <code>torch.cuda.memory._record_memory_history(enabled=None)</code> To visualize the snapshot file, PyTorch has a tool hosted at https://pytorch.org/memory_viz. </p> <p>Reach more : https://pytorch.org/blog/understanding-gpu-memory-1/</p>"},{"location":"profiling/#tensorflow","title":"Tensorflow","text":""},{"location":"profiling/#tensorboard","title":"Tensorboard","text":"<p><code>tensorboard</code> and <code>tensorboard-data-server</code> are available as a module : <pre><code>module load python_ML_packages/3.11.8-cpu\ntensorboard --bind_all --logdir=./tensorboard-log-dir\n</code></pre> or  <pre><code>module load python_ML_packages/3.9.5-gpu\ntensorboard --bind_all --logdir=./tensorboard-log-dir\n</code></pre></p> <p>Read more : https://uppmax.github.io/uppmax4DL/tensorboard/</p>"},{"location":"rpackages/","title":"Installing R packages on Bianca","text":""},{"location":"rpackages/#first-check-if-package-is-already-in-r_packagesxy0","title":"First check if package is already in R_packages/X.Y.0","text":"<ul> <li>You can quickly check if your package is there by:</li> </ul> <p><code>$ ml R_packages/4.1.1</code></p> <p>Then grep for some package, in this case \"glmnet\".</p> <pre><code>$ ls -l $R_LIBS_SITE | grep glmnet\ndr-xr-sr-x  9 douglas sw  4096 Sep  6  2021 EBglmnet\ndr-xr-sr-x 11 douglas sw  4096 Nov 11  2021 glmnet\ndr-xr-sr-x  8 douglas sw  4096 Sep  7  2021 glmnetcr\ndr-xr-sr-x  7 douglas sw  4096 Sep  7  2021 glmnetUtils\n</code></pre>"},{"location":"rpackages/#install-steps","title":"Install steps","text":""},{"location":"rpackages/#install-on-rackham","title":"Install on Rackham","text":"<ul> <li>R on UPPMAX course</li> <li>note First decide on wich R version it should be based on and load that R_packages module.</li> <li>If not stated otherwise, your installation will end up in your <code>~/R</code> directory</li> </ul>"},{"location":"rpackages/#methods","title":"Methods","text":"<ul> <li> <p>automatic download and install from CRAN</p> <ul> <li>https://uppmax.github.io/bianca_workshop/rpackages/#automatical-download-and-install-from-cran</li> </ul> </li> <li> <p>automatic download and install from GitHub</p> <ul> <li>https://uppmax.github.io/bianca_workshop/rpackages/#automatic-download-and-install-from-github</li> </ul> </li> <li> <p>manual download and install</p> <ul> <li>https://uppmax.github.io/bianca_workshop/rpackages/#manual-download-and-install</li> <li>NOTE that if you install a package this way, you need to handle any dependencies yourself.<ul> <li>For instance you might get use of our modules  </li> </ul> </li> </ul> </li> </ul>"},{"location":"rpackages/#transfer-to-wharf","title":"Transfer to wharf","text":"<ul> <li>You may transfer the whole R library (in you home folder)  </li> <li>or select the directory(-ies) related to you new installation<ul> <li>note there may be more than one directory</li> </ul> </li> </ul>"},{"location":"rpackages/#move-package-to-local-bianca-r-package-path","title":"Move package to local Bianca R package path","text":"<ul> <li>Sync or move the R directory or the specific folders to ~/R </li> </ul>"},{"location":"rpackages/#test-your-installation","title":"Test your installation","text":"<ul> <li>Start an R session and load the new package</li> </ul>"},{"location":"rpackages/#example-install-tidycmprsk","title":"Example \u2014 Install Tidycmprsk","text":"<p>tidycmprsk on GitHub</p> <p>Info</p> <p>The tidycmprsk package provides an intuitive interface for working with the competing risk endpoints. The package wraps the cmprsk package, and exports functions for univariate cumulative incidence estimates with cuminc() and competing risk regression with crr().</p>"},{"location":"rpackages/#install-on-rackham_1","title":"Install on Rackham","text":"<p>You can install this for yourself by beginning on rackham. Do</p> <p><pre><code>module load R_packages/4.1.1\n</code></pre> and then, within R, do</p> <pre><code>install.packages('tidycmprsk')\n</code></pre> <p>You will see two questions to answer yes to:</p> <pre><code>Warning in install.packages(\"tidycmprsk\") :\n      'lib = \"/sw/apps/R_packages/4.1.1/rackham\"' is not writable\n    Would you like to use a personal library instead? (yes/No/cancel) yes\n</code></pre> <p>and</p> <pre><code>Would you like to create a personal library\n    '~/R/x86_64-pc-linux-gnu-library/4.1'\n    to install packages into? (yes/No/cancel) yes\n</code></pre> <p>This will then to an extended installation process that also does some updates.  This creates a directory ~/R that contains the installations and updates of R packages.</p>"},{"location":"rpackages/#transfer-to-the-wharf","title":"Transfer to the Wharf","text":"<p>After installation, the next step is to copy the contents of this directory over to bianca so that it is the same directory within your bianca home directory.</p> <p>Make sure you are in your home directory. Then connect to the bianca wharf.  Replace the name and project with your bianca user name and project.</p> <pre><code>sftp douglas-sens2017625@bianca-sftp\n</code></pre> <p>You log in here like you log into bianca: the first password is your password followed by the 6-digit authenticator code, the second password (if required for you) is only your password.</p> <p>Once sftp has connected, the contents of the current directory can be listed with</p> <pre><code>dir\n</code></pre> <p>It should look like this:</p> <pre><code>sftp&gt; dir\ndouglas-sens2017625\n</code></pre> <p>Now <code>cd</code> to this directory, which is your wharf directory within your project.</p> <pre><code>sftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> <p>If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <p>Now, upload your (whole) <code>R</code> directory here.</p> <pre><code>sftp&gt; put -r R\n</code></pre> <p>This will take a while to upload all the files. When it has completed, quit.</p> <pre><code>sftp&gt; quit\n</code></pre> <ul> <li>Now, log into bianca using the shell, or using the web interface and start a terminal. </li> <li>Once you have a bianca shell, change to your wharf directory within your project.  Replace my user and project with yours.</li> </ul> <pre><code>cd /proj/sens2017625/nobackup/wharf/douglas/douglas-sens2017625\n</code></pre> <p>Within this directory should be your R directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ ls -l\ntotal 1892\ndrwxrwxr-x  3 douglas douglas    4096 Mar  2 14:27 R\n</code></pre>"},{"location":"rpackages/#sync-from-wharf-to-home-directory","title":"Sync from Wharf to Home directory","text":"<ul> <li>Now sync this to your home directory:</li> </ul> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ rsync -Pa R ~/\n</code></pre>"},{"location":"rpackages/#start-an-r-session-and-load-the-new-package","title":"Start an R session and load the new package","text":"<p>To use R_packages/4.1.1 with these new installations/updates, change to the directory you want to work in, load the R_packages/4.1.1 module.  Substitute your directory for my example directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ cd /proj/sens2017625/nobackup/douglas/\n    [douglas@sens2017625-bianca douglas]$ module load R_packages/4.1.1\n</code></pre> <p>Then start R, and load the new package.</p> <pre><code>[douglas@sens2017625-bianca douglas]$ R\n</code></pre> <pre><code>    R version 4.1.1 (2021-08-10) -- \"Kick Things\"\n    Copyright (C) 2021 The R Foundation for Statistical Computing\n    ....\n    Type 'demo()' for some demos, 'help()' for on-line help, or\n    'help.start()' for an HTML browser interface to help.\n    Type 'q()' to quit R.\n\n    &gt; library(tidycmprsk)\n    &gt;\n</code></pre>"},{"location":"rpackages_copy/","title":"Installing R packages on Bianca","text":"<p>R on UPPMAX course</p>"},{"location":"rpackages_copy/#what-is-a-package-really","title":"What is a package, really?","text":"<ul> <li> <p>An R package is essentialy a contained folder and file structure containing R code (and possibly C/C++ or other code) and other files relevant for the package e.g. documentation(vignettes), licensing and configuration files. </p> </li> <li> <p>Let us look at a very simple example </p> </li> </ul> <pre><code>   $ git clone git@github.com:MatPiq/R_example.git\n\n   $ cd R_example\n\n   $ tree\n   .\n   \u251c\u2500\u2500 DESCRIPTION\n   \u251c\u2500\u2500 NAMESPACE\n   \u251c\u2500\u2500 R\n   \u2502   \u2514\u2500\u2500 hello.R\n   \u251c\u2500\u2500 man\n   \u2502   \u2514\u2500\u2500 hello.Rd\n   \u2514\u2500\u2500 r_example.Rproj\n</code></pre>"},{"location":"rpackages_copy/#installing-your-own-packages","title":"Installing your own packages","text":"<p>Sometimes you will need R packages that are not already installed. The solution to this is to install your own packages.  - These packages will usually come from CRAN (https://cran.r-project.org/) - the Comprehensive R Archive Network, or - sometimes from other places, like GitHub or R-Forge</p> <p>Here we will look at installing R packages with automatic download and with manual download. It is also possible to install from inside Rstudio. </p>"},{"location":"rpackages_copy/#methods","title":"Methods","text":"<ul> <li>setup (first time)</li> <li>automatic download and install from CRAN</li> <li>automatic download and install from GitHub</li> <li>manual download and install</li> </ul>"},{"location":"rpackages_copy/#setup-first-time","title":"setup (first time)","text":"<p>https://uppmax.github.io/bianca_workshop/rpackages/#setup</p> <ul> <li> <p>We need to create a place for the own-installed packages to be and to tell R where to find them. The initial setup only needs to be done once, but separate package directories need to be created for each R version used.</p> </li> <li> <p>R reads the <code>$HOME/.Renviron</code> file to setup its environment. It should be created by R on first run, or you can create it with the command: touch $HOME/.Renviron</p> </li> </ul> <p>NOTE: In this example we are going to assume you have chosen to place the R packages in a directory under your home directory. As mentioned, you will need separate ones for each R version.</p> <p>If you have not yet installed any packages to R yourself, the environment file should be empty and you can update it like this: </p> <pre><code>    echo R_LIBS_USER=\\\"$HOME/R-packages-%V\\\" &gt; ~/.Renviron\n</code></pre> <p>If it is not empty, you can edit <code>$HOME/.Renviron</code> with your favorite editor so that <code>R_LIBS_USER</code> contain the path to your chosen directory for own-installed R packages. It should look something like this when you are done:</p> <pre><code>    R_LIBS_USER=\"/home/u/user/R-packages-%V\"\n</code></pre> <p>| NOTE: Replace <code>/home/u/user</code> with the value of <code>$HOME</code>. Run <code>echo $HOME</code> to see its value. | NOTE: The <code>%V</code> should be written as-is, it's substituted at runtime with the active R version.</p> <p>For each version of R you are using, create a directory matching the pattern used in <code>.Renviron</code> to store your packages in. This example is shown for R version 4.0.4:</p> <pre><code>    mkdir -p $HOME/R-packages-4.0.4\n</code></pre>"},{"location":"rpackages_copy/#automatical-download-and-install-from-cran","title":"Automatical download and install from CRAN","text":"<p>https://uppmax.github.io/bianca_workshop/rpackages/#automatical-download-and-install-from-cran</p> <p>Note</p> <p>You find a list of packages in CRAN (https://cran.r-project.org/) and a list of repos here: https://cran.r-project.org/mirrors.html </p> <ul> <li>Please choose a location close to you when picking a repo. </li> </ul> From command lineFrom inside R <pre><code>R --quiet --no-save --no-restore -e \"install.packages('&lt;r-package&gt;', repos='&lt;repo&gt;')\"\n</code></pre> <pre><code>install.packages('&lt;r-package&gt;', repos='&lt;repo&gt;')\n</code></pre> <p>In either case, the dependencies of the package will be downloaded and installed as well. </p>"},{"location":"rpackages_copy/#automatic-download-and-install-from-github","title":"Automatic download and install from GitHub","text":"<p>https://uppmax.github.io/bianca_workshop/rpackages/#automatic-download-and-install-from-github</p> <p>If you want to install a package that is not on CRAN, but which do have a GitHub page, then there is an automatic way of installing, but you need to handle prerequsites yourself by installing those first.  -  It can also be that the package is not in as finished a state as those on CRAN, so be careful. </p> <p>Note</p> <p>To install packages from GitHub directly, from inside R, you first need to install the devtools package. Note that you only need to install this once. </p> <p>This is how you install a package from GitHub, inside R:</p> <pre><code>    install.packages(\"devtools\")   # ONLY ONCE\n    devtools::install_github(\"DeveloperName/package\")\n</code></pre>"},{"location":"rpackages_copy/#manual-download-and-install","title":"Manual download and install","text":"<p>https://uppmax.github.io/bianca_workshop/rpackages/#manual-download-and-install</p> <p>If the package is not on CRAN or you want the development version, or you for other reason want to install a package you downloaded, then this is how to install from the command line: </p> <pre><code>    R CMD INSTALL -l &lt;path-to-R-package&gt;/R-package.tar.gz\n</code></pre> <p>NOTE that if you install a package this way, you need to handle any dependencies yourself. </p> <p>Note</p> <p>Places to look for R packages</p> <ul> <li>CRAN (https://cran.r-project.org/)</li> <li>R-Forge (https://r-forge.r-project.org/)</li> <li>Project's own GitHub page</li> <li>etc.</li> </ul>"},{"location":"rpackages_copy/#example-install-tidycmprsk","title":"Example \u2014 Install Tidycmprsk","text":"<p>tidycmprsk on GitHub</p> <p>Info</p> <p>The tidycmprsk package provides an intuitive interface for working with the competing risk endpoints. The package wraps the cmprsk package, and exports functions for univariate cumulative incidence estimates with cuminc() and competing risk regression with crr().</p>"},{"location":"rpackages_copy/#install-on-rackham","title":"Install on Rackham","text":"<p>You can install this for yourself by beginning on rackham. Do</p> <p><pre><code>module load R_packages/4.1.1\n</code></pre> and then, within R, do</p> <pre><code>install.packages('tidycmprsk')\n</code></pre> <p>You will see two questions to answer yes to:</p> <pre><code>Warning in install.packages(\"tidycmprsk\") :\n      'lib = \"/sw/apps/R_packages/4.1.1/rackham\"' is not writable\n    Would you like to use a personal library instead? (yes/No/cancel) yes\n</code></pre> <p>and</p> <pre><code>Would you like to create a personal library\n    '~/R/x86_64-pc-linux-gnu-library/4.1'\n    to install packages into? (yes/No/cancel) yes\n</code></pre> <p>This will then to an extended installation process that also does some updates.  This creates a directory ~/R that contains the installations and updates of R packages.</p>"},{"location":"rpackages_copy/#transfer-to-the-wharf","title":"Transfer to the Wharf","text":"<p>After installation, the next step is to copy the contents of this directory over to bianca so that it is the same directory within your bianca home directory.</p> <p>Make sure you are in your home directory. Then connect to the bianca wharf.  Replace the name and project with your bianca user name and project.</p> <pre><code>sftp douglas-sens2017625@bianca-sftp\n</code></pre> <p>You log in here like you log into bianca: the first password is your password followed by the 6-digit authenticator code, the second password (if required for you) is only your password.</p> <p>Once sftp has connected, the contents of the current directory can be listed with</p> <pre><code>dir\n</code></pre> <p>It should look like this:</p> <pre><code>sftp&gt; dir\ndouglas-sens2017625\n</code></pre> <p>Now <code>cd</code> to this directory, which is your wharf directory within your project.</p> <pre><code>sftp&gt; cd douglas-sens2017625/\nsftp&gt; dir\nsftp&gt;\n</code></pre> <p>If you have not uploaded anything to your wharf, this will be empty. It might have a few things in it.</p> <p>Now, upload your (whole) <code>R</code> directory here.</p> <pre><code>sftp&gt; put -r R\n</code></pre> <p>This will take a while to upload all the files. When it has completed, quit.</p> <pre><code>sftp&gt; quit\n</code></pre> <ul> <li>Now, log into bianca using the shell, or using the web interface and start a terminal. </li> <li>Once you have a bianca shell, change to your wharf directory within your project.  Replace my user and project with yours.</li> </ul> <pre><code>cd /proj/sens2017625/nobackup/wharf/douglas/douglas-sens2017625\n</code></pre> <p>Within this directory should be your R directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ ls -l\ntotal 1892\ndrwxrwxr-x  3 douglas douglas    4096 Mar  2 14:27 R\n</code></pre>"},{"location":"rpackages_copy/#sync-from-wharf-to-home-directory","title":"Sync from Wharf to Home directory","text":"<ul> <li>Now sync this to your home directory:</li> </ul> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ rsync -Pa R ~/\n</code></pre>"},{"location":"rpackages_copy/#start-an-r-session-and-load-the-new-package","title":"Start an R session and load the new package","text":"<p>To use R_packages/4.1.1 with these new installations/updates, change to the directory you want to work in, load the R_packages/4.1.1 module.  Substitute your directory for my example directory.</p> <pre><code>[douglas@sens2017625-bianca douglas-sens2017625]$ cd /proj/sens2017625/nobackup/douglas/\n    [douglas@sens2017625-bianca douglas]$ module load R_packages/4.1.1\n</code></pre> <p>Then start R, and load the new package.</p> <pre><code>[douglas@sens2017625-bianca douglas]$ R\n</code></pre> <pre><code>    R version 4.1.1 (2021-08-10) -- \"Kick Things\"\n    Copyright (C) 2021 The R Foundation for Statistical Computing\n    ....\n    Type 'demo()' for some demos, 'help()' for on-line help, or\n    'help.start()' for an HTML browser interface to help.\n    Type 'q()' to quit R.\n\n    &gt; library(tidycmprsk)\n    &gt;\n</code></pre>"},{"location":"slurm-intro/","title":"Introduction to compute nodes","text":""},{"location":"slurm-intro/#submitting-jobs","title":"Submitting jobs","text":"<p>Objectives</p> <ul> <li>This is a short introduction in how to reach the calculation nodes</li> </ul>"},{"location":"slurm-intro/#slurm-sbatch-the-job-queue","title":"Slurm, sbatch, the job queue","text":"<ul> <li> <p>Problem: 1000 users, 300 nodes, 4.5k cores</p> <ul> <li>Need a queue:</li> </ul> </li> <li> <p>Slurm is a jobs scheduler</p> </li> <li> <p>Plan your job and but in the slurm job batch (sbatch)</p> <ul> <li><code>sbatch &lt;flags&gt; &lt;program&gt;</code> or  <code>sbatch &lt;job script&gt;</code></li> </ul> </li> </ul>"},{"location":"slurm-intro/#jobs","title":"Jobs","text":"<ul> <li>Job = what happens during booked time</li> <li>Described in a Bash script file<ul> <li>Slurm parameters (flags)</li> <li>Load software modules</li> <li>(Move around file system)</li> <li>Run programs</li> <li>(Collect output)</li> </ul> </li> <li>... and more</li> </ul>"},{"location":"slurm-intro/#slurm-parameters","title":"Slurm parameters","text":"<ul> <li>1 mandatory setting for jobs:<ul> <li>Which compute project? (<code>-A</code>)</li> </ul> </li> <li>3 settings you really should set:<ul> <li>Type of queue? (<code>-p</code>)<ul> <li>core, node, (for short development jobs and tests: devcore, devel)</li> </ul> </li> <li>How many cores? (<code>-n</code>)<ul> <li>up to 16 for core job</li> </ul> </li> <li>How long at most? (<code>-t</code>)</li> </ul> </li> <li>If in doubt:<ul> <li><code>-p core</code></li> <li><code>-n 1</code></li> <li><code>-t 10-00:00:00</code></li> </ul> </li> </ul> <ul> <li>Where should it run? (<code>-p node</code> or <code>-p core</code>)</li> <li>Use a whole node or just part of it?<ul> <li>1 node = 16 cores</li> <li>1 hour walltime = 16 core hours = expensive<ul> <li>Waste of resources unless you have a parallel program or need all the memory, e.g. 128 GB per node</li> </ul> </li> </ul> </li> <li>Default value: core</li> </ul>"},{"location":"slurm-intro/#interactive-jobs","title":"Interactive jobs","text":"<ul> <li>Most work is most effective as submitted jobs, but e.g. development needs responsiveness</li> <li>Interactive jobs are high-priority but limited in <code>-n</code> and <code>-t</code></li> <li>Quickly give you a job and logs you in to the compute node</li> <li>Require same Slurm parameters as other jobs</li> </ul>"},{"location":"slurm-intro/#try-interactive","title":"Try interactive","text":"<pre><code>$ interactive -A uppmax2025-3-5 -p core -n 1 -t 10:00\n</code></pre> <ul> <li>Which node are you on?</li> <li>Logout with <code>&lt;Ctrl&gt;-D</code> or <code>logout</code></li> </ul>"},{"location":"slurm-intro/#a-simple-job-script-template","title":"A simple job script template","text":"<pre><code>#!/bin/bash\n\n#SBATCH -A uppmax2025-3-5  # Project ID\n\n#SBATCH -p devcore  # Asking for cores (for test jobs and as opposed to multiple nodes) \n\n#SBATCH -n 1  # Number of cores\n\n#SBATCH -t 00:10:00  # Ten minutes\n\n#SBATCH -J Template_script  # Name of the job\n\n# go to some directory\n\ncd /proj/uppmax2025-3-5/\npwd -P\n\n# load software modules\n\nmodule load python3/3.12.7\nmodule list\n\n# do something\n\necho Hello world!  \n</code></pre>"},{"location":"slurm-intro/#how-compute-nodes-are-moved-between-project-clusters","title":"How compute nodes are moved between project clusters","text":"<p>The total job queue, made by putting together job queues of all project clusters, is monitored, and acted upon, by an external program, named meta-scheduler.</p> <p>In short, this program goes over the following procedure, over and over again:</p> <pre><code>Finds out where all the compute nodes are: on a specific project cluster or yet unallocated.\nReads status reports from all compute nodes, about all their jobs, all their compute nodes, and all their active users.\nAre there unallocated compute nodes for all queued jobs?\nOtherwise, try to \"steal\" nodes from project clusters, to get more unallocated compute nodes. This \"stealing\" is done in two steps: a/ \"drain\" a certain node, i.e. disallow more jobs to start on it; b/ remove the compute node from the project cluster, if no jobs are running on the node.\nUse all unallocated nodes to create new compute nodes. Jobs with a higher priority get compute nodes first.\n</code></pre>"},{"location":"slurm-intro/#other-slurm-tools","title":"Other Slurm tools","text":"<ul> <li><code>squeue</code> \u2014 quick info about jobs in queue</li> <li><code>jobinfo</code> \u2014 detailed info about jobs</li> <li><code>finishedjobinfo</code> \u2014 summary of finished jobs</li> <li><code>jobstats</code> \u2014 efficiency of booked resources</li> <li><code>bianca_combined_jobinfo</code></li> </ul> <p>Objectives</p> <ul> <li>We'll briefly get overviews over <ul> <li>software tools on UPPMAX</li> <li>databases</li> </ul> </li> <li>Introduction quide for installing own software or packages</li> <li>Very short introduction to developing old programs</li> </ul> <p>Keypoints</p> <ul> <li>You are always in the login node unless you:<ul> <li>start an interactive session</li> <li>start a batch job</li> </ul> </li> <li>Slurm is a job scheduler<ul> <li>add flags to describe your job.</li> </ul> </li> <li>There is a job walltime limit of ten days (240 hours).</li> </ul>"},{"location":"slurm/","title":"More about Slurm","text":"<p>Link to Slurm session in Intro to UPPMAX course</p>"},{"location":"summary/","title":"Summary","text":""},{"location":"summary/#next-steps","title":"Next steps","text":"<ul> <li>Q/A session</li> <li>Pick topics from Extra material or ask more general questions, see below summary</li> <li>Exercises ready in the end of the week.</li> </ul>"},{"location":"summary/#todays-topics","title":"Today's topics","text":"<ul> <li> <p>Overview of UPPMAX systems</p> <ul> <li> <p>Several clusters, like </p> <ul> <li>Bianca</li> <li>Rackham</li> </ul> </li> <li> <p>Login and compute nodes</p> </li> </ul> </li> <li> <p>Logging in</p> <ul> <li>ThinLinc, a must for graphics</li> <li>ssh, may be faster if you just use command-line</li> <li>VPN</li> <li>2FA</li> </ul> </li> <li> <p>Command-line intro</p> <ul> <li>navigation </li> <li>aliases</li> <li>chmod</li> </ul> </li> <li> <p>Module system and workflows</p> <ul> <li> <p>modules adds paths to tools</p> <ul> <li>module load ...</li> <li>bioinfo-tools may be required to load you tool</li> </ul> </li> <li> <p>workflows</p> </li> </ul> </li> <li> <p>Transfering files</p> <ul> <li>The \"WHARF\" works like a dock at the harbour.</li> <li> <p>There are several ways to use the wharf to transfer files</p> </li> <li> <p>copy</p> </li> <li>transit server</li> <li>rsync, scp/sftp</li> </ul> </li> <li> <p>NAISS-SENS and sensitive data</p> <ul> <li>Sensitive personal data</li> <li>Pseudonymisation and anonymisation</li> <li>Apply for project</li> </ul> </li> <li> <p>Introduction to compute nodes</p> <ul> <li> <p>Submitting jobs</p> <ul> <li>Slurm, sbatch, the job queue</li> <li>Jobs</li> <li>Slurm parameters</li> <li> <p>Interactive jobs </p> <ul> <li>(allows you to work interactively with commandline and yopur tools but on a compute node.</li> </ul> </li> </ul> </li> <li> <p>How compute nodes are moved between project clusters</p> </li> <li> <p>Other Slurm tools</p> </li> </ul> </li> <li> <p>Software and package installation</p> <ul> <li>Install software yourself</li> <li> <p>Packages and libraries to scripting programs</p> <ul> <li>Conda</li> <li>Python packages with pip</li> <li>R packages</li> <li>Julia packages</li> </ul> </li> <li> <p>\"Containers\"</p> <ul> <li>Singularity</li> <li>Docker</li> </ul> </li> <li> <p>Build from source</p> </li> <li>Summary about the Bianca Hardware</li> </ul> </li> </ul>"},{"location":"tensorboard/","title":"Running TensorBoard on Snowy","text":"<p>Info</p> <ul> <li>TensorBoard can be used to visualise, debug, and profile your model.</li> <li>TensorBoard is accessed from a web browser. You may start it in an interactive session and access it via an ssh-tunnel or ThinLinc.</li> </ul>"},{"location":"tensorboard/#1-connect-to-rackham-using-thinlinc","title":"1. Connect to Rackham using ThinLinc.","text":"<p>You may download the ThinLinc client from here: https://www.cendio.com/thinlinc/download</p> <p>You may also use the web browser to connect: https://rackham-gui.uppmax.uu.se You need to set up 2FA for the ThinLinc web.</p>"},{"location":"tensorboard/#2-open-a-terminal-in-thinlinc-and-ask-for-an-interactive-session-to-snowy","title":"2. Open a terminal in ThinLinc and ask for an interactive session to Snowy.","text":"<pre><code>salloc -A naiss2023-22-247 -p node -N 1 -M snowy --gpus=1 --gpus-per-node=1 -t 04:00:00\n</code></pre> <p>Note: We have a magnetic 4-node reservation for today: <code>naiss2023-22-247_1</code>.</p> <p>Is the GPU visible? <pre><code>echo $CUDA_VISIBLE_DEVICES\n</code></pre></p>"},{"location":"tensorboard/#3-load-the-needed-modules","title":"3. Load the needed modules","text":"<p><pre><code>module load python_ML_packages/3.9.5-gpu\npip list\n</code></pre> OR</p> <pre><code>module use /sw/EasyBuild/snowy-gpu/modules/all\nmodule load TensorFlow/2.5.0-fosscuda-2020b\n</code></pre> <p>TensorBoard can be started from the command line as <code>tensorboard</code>. You need to specify a directory for the logs using the <code>--logdir</code> option.</p> <p>Example: <pre><code>tensorboard --bind_all --logdir=./tensorboard-log-dir\n...\nTensorBoard 2.5.0 at http://s???.uppmax.uu.se:6006/\n</code></pre> Change the path &amp; name <code>./tensorboard-log-dir</code> as you wish. Each running job should have its own directory for TensorBoard to parse the logs correctly.</p> <p>By default, TensorBoard will attach the port <code>6006</code> to localhost. If it's taken, it will chose another one. You may see the chosen port in the output when you start TensorBoard from the command line.</p>"},{"location":"tensorboard/#4-accesss-tensorboard","title":"4. Accesss TensorBoard","text":""},{"location":"tensorboard/#solution-1-using-ssh-tunnelling","title":"Solution 1: Using ssh-tunnelling","text":"<p><pre><code>ssh -L 8080:s???.uppmax.uu.se:6006 user@rackham.uppmax.uu.se\n</code></pre> Update the port if needed.</p> <p>Next, on your computer open a web browser and visit http://localhost:8080. You should now see the Tensorboard UI.</p>"},{"location":"tensorboard/#solution-2-using-thinlinc","title":"Solution 2: Using ThinLinc","text":"<ul> <li>Launch a web browser on ThinLinc.</li> <li>Visit <code>http://s???.uppmax.uu.se:6006</code> or <code>localhost:8080</code>. Update the local port, in this example <code>8080</code>, and the snowy node <code>s???</code> as needed.</li> </ul>"},{"location":"tensorboard/#where-to-go-from-here","title":"Where to go from here?","text":"<p>Have a look at this getting started tutorial from TensorFlow: https://www.tensorflow.org/tensorboard/get_started.</p> <p>Warning</p> <p>Limit the memory needs of your application. What is a suitable <code>TF_FORCE_GPU_ALLOW_GROWTH</code>? Read more on this and other memory growth pointers at https://www.tensorflow.org/guide/gpu.</p>"},{"location":"transfer/","title":"Transfer","text":"<p>Objectives</p> <ul> <li>We'll go through the methods to transfer files</li> <li>wharf</li> <li>transit server</li> <li>rsync, scp/sftp</li> <li>pros/cons of different solutions</li> </ul> <p>Warning</p> <p>It is important to keep the entire chain of transferring the data secure</p>"},{"location":"transfer/#how-does-it-work","title":"How does it work?","text":""},{"location":"transfer/#the-wharf","title":"The Wharf","text":"<p>Wharf is a harbour dock</p> <ul> <li>The Wharf area can be reached from both Bianca and any other place on Bianca.</li> <li>Therefore, it serves as a bridge between Internet and Bianca.</li> </ul>"},{"location":"transfer/#data-transfers","title":"Data transfers:","text":"<ul> <li>https://www.uppmax.uu.se/support/user-guides/bianca-user-guide/ <ul> <li>section 3: Transfer files to and from Bianca</li> </ul> </li> </ul>"},{"location":"transfer/#the-wharf-location","title":"The wharf location","text":"<ul> <li> <p>The path to this folder, once you are logged into your project's cluster, is:</p> <p><code>/proj/&lt;projid&gt;/nobackup/wharf/&lt;username&gt;/&lt;username&gt;-&lt;projid&gt;</code> E.g. <code>/proj/sens2016999/nobackup/wharf/myuser/myuser-sens2016999</code></p> </li> <li> <p>To transfer data from Bianca, copy the files you want to transfer here.</p> </li> <li> <p>To get the files transferred to the wharf area from outside, move the files to you project folde or home folder.</p> </li> <li> <p>Please note that in the wharf you only have access to upload your files to the directory that is named:    <code>&lt;username&gt;-&lt;projid&gt;</code>    e.g.    <code>myuser-sens2016999</code></p> </li> </ul>"},{"location":"transfer/#methods","title":"Methods","text":"<ul> <li>Using standard sftp client</li> <li>Some other sftp clients</li> <li>Mounting the wharf on your local computer</li> <li>Transit Server from Rackham</li> </ul>"},{"location":"transfer/#using-standard-sftp-client-command-line","title":"Using standard sftp client (command line)","text":"<p>https://www.uppmax.uu.se/support/user-guides/basic-sftp-commands/</p> <p><code>$ sftp -q &lt;username&gt;-&lt;projid&gt;@bianca-sftp.uppmax.uu.se</code>  Ex. <code>$ sftp -q myuser-sens2016999@bianca-sftp.uppmax.uu.se</code></p> <p>Notice the different host name from before!</p> <p>The <code>-q</code> flag is to be quiet (not showing the banner intended to help someone trying to ssh to the host), if your client does not support it, you can just skip it.</p> <p>As password you use your normal UPPMAX password directly followed by the six digits from the second factor application from step 1.</p> <p>Ex. if your password is \"VerySecret\" and the second factor code is 123 456 you would type VerySecret123456 as the password in this step.</p> <p>Once connected you will have to type the sftp commands to upload/download files. Have a look at the Basic SFTP commands guide to get started with it.</p> <p>Please note that in the wharf you only have access to upload your files to the directory that is named:</p> <p><code>&lt;username&gt;-&lt;projid&gt;</code> e.g. <code>myuser-sens2016999</code></p> <p>so you will want to cd to that directory the first thing you do.</p> <p><code>sftp&gt; cd myuser-sens2016999</code></p> <p>Alternatively, you can specify this at the end of the sftp command, so that you will always end up in the correct folder directly.</p> <p><code>$ sftp -q &lt;username&gt;-&lt;projid&gt;@bianca-sftp.uppmax.uu.se:&lt;username&gt;-&lt;projid&gt;</code> E.g. <code>$ sftp -q myuser-sens2016999@bianca-sftp.uppmax.uu.se:myuser-sens2016999</code></p> <ul> <li><code>sftp</code> supports a recursive flag (<code>put -r</code>), but it seems to be very sensitive to combinations of different sftp servers and clients, so be warned... a bit further down you can see a rough solution for bulk transfers.</li> </ul> <p>Info</p> <p>Bulk recursive transfer with only standard sftp client</p> <ul> <li>It seems to be rather common with directory structures with symbolic links inside the directories that you should transfer. </li> <li>This is a very simple solution to copy everything in a specific folder (and follow symbolic links) to the wharf.</li> </ul> <p><pre><code>==============\n~/sftp-upload.sh\n==============\n#!/bin/bash\n#sftp-upload.sh\nfind $* -type d | awk '{print \"mkdir\",\"\\\"\"$0\"\\\"\"}' \nfind $* -type f | awk '{print \"put\",\"\\\"\"$0\"\\\"\",\"\\\"\"$0\"\\\"\" }' \nfind $* -type l | awk '{print \"put\",\"\\\"\"$0\"\\\"\",\"\\\"\"$0\"\\\"\" }' \n-----------\n</code></pre> With this script you can do:</p> <p><pre><code>cd /home/myuser/glob/testing/nobackup/somedata\n~/sftp-upload.sh *|sftp -oBatchMode=no -b- &lt;username&gt;-&lt;projid&gt;@bianca-sftp.uppmax.uu.se:&lt;username&gt;-&lt;projid&gt;\n</code></pre> The special <code>-b</code> makes the script stop on error.</p>"},{"location":"transfer/#some-other-sftp-client","title":"Some other sftp client","text":"<ul> <li> <p>Please notice that sftp is NOT the same as scp. So be sure to really use a sftp client -- not just a scp client.</p> </li> <li> <p>Also be aware that many sftp clients use reconnects (with a cached version of your password). This will not work for Bianca, because of the second factor! And some try to use multiple connections with the same password, which will fail.</p> </li> <li> <p>So for example with lftp, you need to \"set net:connection_limit 1\". lftp may also defer the actual connection until it's really required unless you end your connect URL with a path.</p> </li> <li> <p>An example command line for lftp would be</p> </li> </ul> <p><code>lftp sftp://&lt;username&gt;-&lt;projname&gt;@bianca-sftp.uppmax.uu.se/&lt;username&gt;-&lt;projname&gt;/</code></p>"},{"location":"transfer/#winscp-windows","title":"WinSCP (Windows)","text":"<ul> <li>Does work!</li> <li>Only connect with local computer (not Rackham)</li> </ul>"},{"location":"transfer/#filezilla","title":"Filezilla","text":"<ul> <li>Does work</li> <li>but asks for password everytime</li> <li>Only connect with local computer (not Rackham)</li> </ul>"},{"location":"transfer/#mounting-the-sftp-server-with-sshfs-on-you-local-machine","title":"Mounting the sftp-server with sshfs on you local machine","text":"<p>Mount the wharf on your machine</p> <ul> <li>This is only possible on your own system. </li> <li><code>sshfs</code> allows you to mount the wharf on your own machine. </li> <li>You will be able to copy and work on the data using your own local tools such as cp or vim. </li> <li>Remember that you are neither logged in on the distant server, nor is the data physically on your local disk (until you have copied it).</li> </ul> <p>Warning</p> <ul> <li>UPPMAX doesn't have sshfs client package installed for security reasons. </li> <li>sshfs is available on most Linux distributions: <ul> <li>install the package sshfs on Ubuntu, </li> <li>fuse-sshfs on Fedora, RHEL7/CentOS7 (enable EPEL repository) and RHEL8 (enable codeready-builder repository) / CentOS8 (enable powertools repository).    </li> </ul> </li> </ul>"},{"location":"transfer/#transit","title":"Transit","text":"<ul> <li>To facilitate secure data transfers to, from and within the system for computing on sensitive data (bianca/castor) a service is available via ssh at transit.uppmax.uu.se.</li> <li> <p>You can connect to transit via ssh. Once connected, you should see a short help message. The most important thing there is the <code>mount_wharf</code> command which you can use to mount a project from the bianca wharf</p> </li> <li> <p>Example from Rackham as Rackham session</p> </li> </ul> <p><pre><code>ssh transit\nusername@transit:~$ mount_wharf sens2023531\nMounting wharf (accessible for you only) to /home/&lt;user&gt;/sens2023531\n&lt;user&gt;-sens2023531@bianca-sftp.uppmax.uu.se's password: \n</code></pre> - Enter password + F2A</p> <pre><code>done.\nusername@transit:~$ ls sens2023531/\nusername@transit:~$ \n</code></pre> <ul> <li> <p>Note that your home directory is mounted read-only, any changes you do to your \"local\" home directory (on transit) will be forgotten afterwards.</p> </li> <li> <p>You can use commands like <code>rsync</code>, <code>scp</code> to fetch data and transfer it to your bianca wharf.</p> </li> <li>You can use cp to copy from Rackham to the wharf </li> <li>Remember that you cannot make lasting changes to anything except for mounted wharf directories. Therefore you have to use rsync and scp to tranfer from the wharf to Rackham.</li> <li>The mounted directory will be kept for later sessions.</li> </ul>"},{"location":"transfer/#moving-data-from-transit-to-rackham","title":"Moving data from transit to Rackham","text":"<ul> <li> <p>On Rackham: copy files to Bianca via transit:  <code>scp path/my_files transit:sens2023531/</code></p> </li> <li> <p>On transit: copy files to Bianca from Rackham <code>scp  rackham:path/my_files ~/sens2023531/</code></p> </li> </ul> <p>Keep in mind that project folders on Rackham are not available on transit.</p>"},{"location":"transfer/#moving-data-between-projects","title":"Moving data between projects","text":"<ul> <li>You can use transit to transfer data between projects by mounting the wharfs for the different projects and transferring data with rsync. </li> <li>Note that you may of course only do this if this is allowed (agreements, permissions, etc.)</li> </ul>"},{"location":"transfer/#software-on-transit","title":"Software on Transit","text":"<ul> <li> <p>While logged in to Transit, you cannot make lasting changes to anything except for mounted wharf directories. However, anything you have added to your Rackham home directory is available on Transit. In addition, some modules are available.</p> </li> <li> <p>For example, if you need to download data from TCGA, log in to Rackham and install the GDC client to your home directory. Then log in to Transit, mount the wharf, and run ./gdc-client.</p> </li> </ul>"},{"location":"transfer/#ngi-deliver","title":"NGI Deliver","text":"<ul> <li>Not covered here but </li> <li>https://www.uppmax.uu.se/support/user-guides/deliver-user-guide/</li> <li>https://www.uppmax.uu.se/support/user-guides/grus-user-guide/</li> </ul> <p>Summary</p> <ul> <li>Make sure you access Bianca from SUNET Network - use VPN, connect from Rackham, use university connection...</li> <li>For simple transfers use SFP to connect to <code>bianca-sftp.uppmax.uu.se</code> - use command line <code>sftp</code> or tools that support SFTP protocol.</li> <li>For <code>rsync</code> - sync files to pre-mounted wharf folder from Rackham or secure local computer.</li> <li>Keep in mind that project folders on Rackham are not available on transit.</li> </ul> <p>keypoints</p> <ul> <li>The \"WHARF\" works like a dock at the harbour.</li> <li>There are several ways to use the wharf to transfer files<ul> <li>copy</li> <li>transit server</li> <li>rsync, scp/sftp</li> </ul> </li> </ul>"}]}